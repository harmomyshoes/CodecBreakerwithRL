{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6720e15-4ea6-4d47-9fe1-40dca2c7acb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c0095-52f9-49e5-a36a-04bffdf2e748",
   "metadata": {},
   "source": [
    "#### 1.Discrete Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fd7c06-a4eb-4b4a-ac74-2eb2da8fc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(action_spec):\n",
    "    \"\"\"Draw a random integer action that fits the BoundedArraySpec.\"\"\"\n",
    "    low  = action_spec.minimum\n",
    "    high = action_spec.maximum\n",
    "    # note: high is inclusive in the BoundedArraySpec, so +1 for randint\n",
    "    return np.random.randint(low, high + 1, size=action_spec.shape, dtype=action_spec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc722b6-c835-482b-ae56-fc8d3a016c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "from env import Env_Discrete\n",
    "import numpy as np\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories.time_step import StepType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06459f27-f9fe-487b-8d42-c02e0cffb629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action spec: BoundedArraySpec(shape=(3,), dtype=dtype('int32'), name='action', minimum=[0 0 0], maximum=[6 4 2])\n",
      "Observation spec: BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n",
      "\n",
      "After reset:\n",
      "  step_type: FIRST\n",
      "  obs      : [-1.5 -1.5 -1.5]\n",
      "  reward   : 0.0\n",
      "  discount : 1.0\n",
      "\n",
      "Stepping 5 times with random actions:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m a \u001b[38;5;241m=\u001b[39m sample_action(a_spec)\n\u001b[1;32m     21\u001b[0m ts \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[0;32m---> 22\u001b[0m st \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMID\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mstep_type \u001b[38;5;241m==\u001b[39m StepType\u001b[38;5;241m.\u001b[39mMID \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m     23\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAST\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mstep_type \u001b[38;5;241m==\u001b[39m StepType\u001b[38;5;241m.\u001b[39mLAST \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m       ts\u001b[38;5;241m.\u001b[39mstep_type)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: action=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → obs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts\u001b[38;5;241m.\u001b[39mreward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step_type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mstep_type \u001b[38;5;241m==\u001b[39m StepType\u001b[38;5;241m.\u001b[39mLAST:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "env = Env_Discrete()\n",
    "\n",
    "# 1) check specs\n",
    "a_spec = env.action_spec()\n",
    "o_spec = env.observation_spec()\n",
    "print(\"Action spec:\", a_spec)\n",
    "print(\"Observation spec:\", o_spec)\n",
    "\n",
    "# 2) reset\n",
    "ts = env.reset()\n",
    "print(\"\\nAfter reset:\")\n",
    "print(\"  step_type:\", \"FIRST\" if ts.step_type == StepType.FIRST else ts.step_type)\n",
    "print(\"  obs      :\", ts.observation)\n",
    "print(\"  reward   :\", ts.reward)\n",
    "print(\"  discount :\", ts.discount)\n",
    "\n",
    "# 3) take 5 random steps\n",
    "print(\"\\nStepping 5 times with random actions:\")\n",
    "for i in range(5):\n",
    "    a = sample_action(a_spec)\n",
    "    ts = env.step(a)\n",
    "    st = (\"MID\" if ts.step_type == StepType.MID else\n",
    "          \"LAST\" if ts.step_type == StepType.LAST else\n",
    "          ts.step_type)\n",
    "    print(f\" Step {i+1:>2}: action={a} → obs={ts.observation}, reward={ts.reward:.4f}, step_type={st}\")\n",
    "    if ts.step_type == StepType.LAST:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50923384-b9e8-47c9-a65c-b96b95e83080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array(1., dtype=float32),\n",
       " 'observation': array([-0.9, -1.3, -3.5], dtype=float32),\n",
       " 'reward': array([   3.8809,    2.9369, -127.6375], dtype=float32),\n",
       " 'step_type': array([1, 1, 1], dtype=int32)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abcf5ee-05c0-45e2-b6ac-e9a6ea054490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   3, 100], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_spec.maximum - a_spec.minimum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ea513e-bfaf-408b-8f42-732ba2ad3efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a_spec.minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d03b7-9765-4b77-999f-b19f68511fee",
   "metadata": {},
   "source": [
    "#### 2.Continouse Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8479f43b-5ff9-4199-bfc2-c0dbf4385248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_continuous(action_spec):\n",
    "    \"\"\"Uniformly sample a float vector in [minimum, maximum].\"\"\"\n",
    "    low, high = action_spec.minimum, action_spec.maximum\n",
    "    return np.random.uniform(low, high, size=action_spec.shape).astype(action_spec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c9358b-4396-4583-9a60-6ca461bba72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action spec: BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='action', minimum=[-2.  3.  1.], maximum=[  2.   5. 100.])\n",
      "Observation spec: BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n",
      "\n",
      "After reset:\n",
      "  step_type: FIRST\n",
      "  obs      : [0. 0. 0.]\n",
      "  reward   : 0.0\n",
      "  discount : 1.0\n",
      "\n",
      "Stepping 5 times with random actions:\n",
      " Step  1: action=[ 1.8997207  4.6012278 89.42736  ] → obs=[ 1.8997207  4.6012278 89.42736  ], reward=-8022.0332, step_type=MID\n",
      " Step  2: action=[-1.957699   4.7816534 73.24402  ] → obs=[-5.7978272e-02  9.3828812e+00  1.6267139e+02], reward=-26550.0215, step_type=MID\n",
      " Step  3: action=[-0.91692525  4.8964863  64.06855   ] → obs=[ -0.9749035  14.279367  226.73993  ], reward=-51615.8477, step_type=MID\n",
      " Step  4: action=[ 0.9561594  3.2213173 55.088135 ] → obs=[-1.8744111e-02  1.7500685e+01  2.8182806e+02], reward=-79733.3281, step_type=MID\n",
      " Step  5: action=[-0.28203082  4.453713   48.00706   ] → obs=[-3.0077493e-01  2.1954397e+01  3.2983511e+02], reward=-109273.2891, step_type=MID\n"
     ]
    }
   ],
   "source": [
    "env = Env_Continue()\n",
    "\n",
    "# 1) check specs\n",
    "a_spec = env.action_spec()\n",
    "o_spec = env.observation_spec()\n",
    "print(\"Action spec:\", a_spec)\n",
    "print(\"Observation spec:\", o_spec)\n",
    "\n",
    "# 2) reset\n",
    "ts = env.reset()\n",
    "print(\"\\nAfter reset:\")\n",
    "print(\"  step_type:\", \"FIRST\" if ts.step_type == StepType.FIRST else ts.step_type)\n",
    "print(\"  obs      :\", ts.observation)\n",
    "print(\"  reward   :\", ts.reward)\n",
    "print(\"  discount :\", ts.discount)\n",
    "\n",
    "# 3) take 5 random steps\n",
    "print(\"\\nStepping 5 times with random actions:\")\n",
    "for i in range(5):\n",
    "    a = sample_continuous(a_spec)\n",
    "    ts = env.step(a)\n",
    "    st = (\"MID\" if ts.step_type == StepType.MID else\n",
    "          \"LAST\" if ts.step_type == StepType.LAST else\n",
    "          ts.step_type)\n",
    "    print(f\" Step {i+1:>2}: action={a} → obs={ts.observation}, reward={ts.reward:.4f}, step_type={st}\")\n",
    "    if ts.step_type == StepType.LAST:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a93f4e-7711-4696-a1c9-0819b3cb4c88",
   "metadata": {},
   "source": [
    "### End Env Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468db71e-16c6-409a-a7af-5af2ca93c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 17:46:31.942278: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-05 17:46:31.944927: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-05 17:46:31.982828: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-05 17:46:31.983861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-05 17:46:32.557477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-05 17:46:33.928505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import discrete_RL_train\n",
    "from discrete_RL_train import discrete_RL_train as RLTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311ba30e-c03d-4f35-92c3-d77faca19bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainner = RLTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c967465-cf26-44d1-b4d2-8c58ed3b9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_env.batch_size = parallel environment number =  6\n",
      "WARNING:tensorflow:From /home/codecrack/ML_Test/Optimiser/discrete_RL_train.py:126: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/codecrack/ML_Test/Optimiser/discrete_RL_train.py:126: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step   0] mean entropy per-dim = 2.0641\n",
      "final reward before udpate: -1000000000.0\n",
      "final reward after udpate: -14.0625\n",
      "updated final_solution= [-1.5 -1.5 -1.5]\n",
      "train_step no.= 1\n",
      "best_solution of this generation= [-1.5 -1.5 -1.5]\n",
      "best step reward= -14.062\n",
      "avg step reward= -102304.055\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.5       -1.5       -1.6       -1.6       -1.7\n",
      " -1.8       -1.8       -1.9       -2.        -2.        -2.1\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.5\n",
      " -2.6       -2.6       -2.7       -2.7       -2.8       -2.8\n",
      " -2.9       -3.        -3.        -3.        -3.1       -3.1\n",
      " -3.1       -3.2       -3.2       -3.3       -3.3       -3.3\n",
      " -3.3       -3.3       -3.4       -3.4       -3.4       -3.4\n",
      " -3.4       -3.4       -3.5       -3.5       -3.5       -3.6000001\n",
      " -3.7       -3.8      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step   1] mean entropy per-dim = 0.0000\n",
      "[step   2] mean entropy per-dim = 0.0000\n",
      "[step   3] mean entropy per-dim = 0.0000\n",
      "[step   4] mean entropy per-dim = 0.0000\n",
      "[step   5] mean entropy per-dim = 0.0000\n",
      "[step   6] mean entropy per-dim = 0.0000\n",
      "[step   7] mean entropy per-dim = 0.0000\n",
      "[step   8] mean entropy per-dim = 0.0000\n",
      "[step   9] mean entropy per-dim = 0.0000\n",
      "[step  10] mean entropy per-dim = 0.0000\n",
      "train_step no.= 11\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  11] mean entropy per-dim = 0.0000\n",
      "[step  12] mean entropy per-dim = 0.0000\n",
      "[step  13] mean entropy per-dim = 0.0000\n",
      "[step  14] mean entropy per-dim = 0.0000\n",
      "[step  15] mean entropy per-dim = 0.0000\n",
      "[step  16] mean entropy per-dim = 0.0000\n",
      "[step  17] mean entropy per-dim = 0.0000\n",
      "[step  18] mean entropy per-dim = 0.0000\n",
      "[step  19] mean entropy per-dim = 0.0000\n",
      "[step  20] mean entropy per-dim = 0.0000\n",
      "train_step no.= 21\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  21] mean entropy per-dim = 0.0000\n",
      "[step  22] mean entropy per-dim = 0.0000\n",
      "[step  23] mean entropy per-dim = 0.0000\n",
      "[step  24] mean entropy per-dim = 0.0000\n",
      "[step  25] mean entropy per-dim = 0.0000\n",
      "[step  26] mean entropy per-dim = 0.0000\n",
      "[step  27] mean entropy per-dim = 0.0000\n",
      "[step  28] mean entropy per-dim = 0.0000\n",
      "[step  29] mean entropy per-dim = 0.0000\n",
      "[step  30] mean entropy per-dim = 0.0000\n",
      "train_step no.= 31\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  31] mean entropy per-dim = 0.0000\n",
      "[step  32] mean entropy per-dim = 0.0000\n",
      "[step  33] mean entropy per-dim = 0.0000\n",
      "[step  34] mean entropy per-dim = 0.0000\n",
      "[step  35] mean entropy per-dim = 0.0000\n",
      "[step  36] mean entropy per-dim = 0.0000\n",
      "[step  37] mean entropy per-dim = 0.0000\n",
      "[step  38] mean entropy per-dim = 0.0000\n",
      "[step  39] mean entropy per-dim = 0.0000\n",
      "[step  40] mean entropy per-dim = 0.0000\n",
      "train_step no.= 41\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  41] mean entropy per-dim = 0.0000\n",
      "[step  42] mean entropy per-dim = 0.0000\n",
      "[step  43] mean entropy per-dim = 0.0000\n",
      "[step  44] mean entropy per-dim = 0.0000\n",
      "[step  45] mean entropy per-dim = 0.0000\n",
      "[step  46] mean entropy per-dim = 0.0000\n",
      "[step  47] mean entropy per-dim = 0.0000\n",
      "[step  48] mean entropy per-dim = 0.0000\n",
      "[step  49] mean entropy per-dim = 0.0000\n",
      "[step  50] mean entropy per-dim = 0.0000\n",
      "train_step no.= 51\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  51] mean entropy per-dim = 0.0000\n",
      "[step  52] mean entropy per-dim = 0.0000\n",
      "[step  53] mean entropy per-dim = 0.0000\n",
      "[step  54] mean entropy per-dim = 0.0000\n",
      "[step  55] mean entropy per-dim = 0.0000\n",
      "[step  56] mean entropy per-dim = 0.0000\n",
      "[step  57] mean entropy per-dim = 0.0000\n",
      "[step  58] mean entropy per-dim = 0.0000\n",
      "[step  59] mean entropy per-dim = 0.0000\n",
      "[step  60] mean entropy per-dim = 0.0000\n",
      "train_step no.= 61\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  61] mean entropy per-dim = 0.0000\n",
      "[step  62] mean entropy per-dim = 0.0000\n",
      "[step  63] mean entropy per-dim = 0.0000\n",
      "[step  64] mean entropy per-dim = 0.0000\n",
      "[step  65] mean entropy per-dim = 0.0000\n",
      "[step  66] mean entropy per-dim = 0.0000\n",
      "[step  67] mean entropy per-dim = 0.0000\n",
      "[step  68] mean entropy per-dim = 0.0000\n",
      "[step  69] mean entropy per-dim = 0.0000\n",
      "[step  70] mean entropy per-dim = 0.0000\n",
      "train_step no.= 71\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  71] mean entropy per-dim = 0.0000\n",
      "[step  72] mean entropy per-dim = 0.0000\n",
      "[step  73] mean entropy per-dim = 0.0000\n",
      "[step  74] mean entropy per-dim = 0.0000\n",
      "[step  75] mean entropy per-dim = 0.0000\n",
      "[step  76] mean entropy per-dim = 0.0000\n",
      "[step  77] mean entropy per-dim = 0.0000\n",
      "[step  78] mean entropy per-dim = 0.0000\n",
      "[step  79] mean entropy per-dim = 0.0000\n",
      "[step  80] mean entropy per-dim = 0.0000\n",
      "train_step no.= 81\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  81] mean entropy per-dim = 0.0000\n",
      "[step  82] mean entropy per-dim = 0.0000\n",
      "[step  83] mean entropy per-dim = 0.0000\n",
      "[step  84] mean entropy per-dim = 0.0000\n",
      "[step  85] mean entropy per-dim = 0.0000\n",
      "[step  86] mean entropy per-dim = 0.0000\n",
      "[step  87] mean entropy per-dim = 0.0000\n",
      "[step  88] mean entropy per-dim = 0.0000\n",
      "[step  89] mean entropy per-dim = 0.0000\n",
      "[step  90] mean entropy per-dim = 0.0000\n",
      "train_step no.= 91\n",
      "best_solution of this generation= [-1.6 -1.6 -2.5]\n",
      "best step reward= -39.699\n",
      "avg step reward= -1406564.0\n",
      "best_step_index: [0, 1]\n",
      "collect_traj: [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "test_traj [-1.5       -1.6       -1.7       -1.8       -1.9       -2.\n",
      " -2.1       -2.2       -2.3       -2.4       -2.5       -2.6\n",
      " -2.7       -2.8       -2.9       -3.        -3.1       -3.2\n",
      " -3.3       -3.4       -3.5       -3.6000001 -3.7       -3.8\n",
      " -3.9       -4.        -4.1       -4.2       -4.3       -4.4\n",
      " -4.5       -4.6       -4.7       -4.8       -4.9       -5.\n",
      " -5.1       -5.2000003 -5.3       -5.4       -5.5       -5.6\n",
      " -5.7000003 -5.8       -5.9       -6.        -6.1       -6.2000003\n",
      " -6.3       -6.4      ]\n",
      "[step  91] mean entropy per-dim = 0.0000\n",
      "[step  92] mean entropy per-dim = 0.0000\n",
      "[step  93] mean entropy per-dim = 0.0000\n",
      "[step  94] mean entropy per-dim = 0.0000\n",
      "[step  95] mean entropy per-dim = 0.0000\n",
      "[step  96] mean entropy per-dim = 0.0000\n",
      "[step  97] mean entropy per-dim = 0.0000\n",
      "[step  98] mean entropy per-dim = 0.0000\n",
      "[step  99] mean entropy per-dim = 0.0000\n",
      "final_solution= [-1.5 -1.5 -1.5] final_reward= -14.0625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625,\n",
       "  -14.0625],\n",
       " array([-1.5, -1.5, -1.5], dtype=float32),\n",
       " -14.0625)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e35761-9f5f-4100-b429-67b3ec74e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc9262b-0ad5-4470-8580-1eec0fdc4be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(np.square(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78358b12-ff65-41eb-a343-496794c98f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e6f50-9c17-4cd8-8480-0e9d94dff976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
