{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc5246e-a3b8-413a-8a6e-19f419522636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 18:27:14.056672: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-01 18:27:14.059545: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-01 18:27:14.115194: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-01 18:27:14.116530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-01 18:27:15.160991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-01 18:27:17.338343: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from custom_normal_projection_network import NormalProjectionNetwork\n",
    "import os,gc\n",
    "import pygad\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "import time\n",
    "\n",
    "\n",
    "#To limit TensorFlow to a specific set of GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34af6163-34e0-4d13-b7cb-4b372c2f659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the objective f to be maximized\n",
    "N = 3   #This the dimension number\n",
    "state_dim = N\n",
    "act_min = np.array([-2.0, 3.0,  1.0], dtype=np.float32)\n",
    "act_max = np.array([ 2.0, 8.0, 100.0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b47086-ca57-442e-9c58-8915b56a4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#Setting Reward Function\n",
    "def f(x):\n",
    "    x1, x2, x3 = x[0], x[1], x[2]\n",
    "    term1 = x1**2 - 10*math.cos(2*math.pi*x1)\n",
    "    term2 = (x2-5)**2 - 10*math.cos(2*math.pi*(x2-5))\n",
    "    term3 = (x3-50)**2 - 10*math.cos(2*math.pi*(x3-100))\n",
    "    return -(term1 + term2 + term3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a4d1e3-b5f3-45ad-9cb8-c40b59dd8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 [ 0.  5. 10.]\n"
     ]
    }
   ],
   "source": [
    "#Set initial x-values for all three algorithms\n",
    "x0_reinforce = np.array([0.0, 5.0, 10.0],dtype=np.float32)\n",
    "final_reward   = -1e9\n",
    "final_solution = x0_reinforce.copy()    # so it always exists\n",
    "\n",
    "sub_episode_length = 5 #number of time_steps in a sub-episode. \n",
    "sub_episode_num_in_single_env = 4\n",
    "episode_length = sub_episode_length*sub_episode_num_in_single_env  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 3  #Number of parallel environments, each environment is used to generate an episode\n",
    "print('x0', x0_reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913bb0d4-04ac-4df3-83f1-03e7a811eccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sub_episodes used for a single param update: 12\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "#Set hyper-parameters for REINFORCE-OPT\n",
    "generation_num = 300  #number of theta updates for REINFORCE-IP, also serves as the number\n",
    "                      #of generations for GA, and the number of iterations for particle swarm optimization\n",
    "\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d30aa4-0aa4-4384-ac02-2da9f04a1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.001, C=50000)   \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "#opt = tf.keras.optimizers.legacy.Adam( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7211c5-25ad-4d0f-bb62-02bb0563ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed34e79-22ff-4c88-bb03-9aa4e0e097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(x):\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8552c58-f46e-43fb-a679-691729ecc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                            shape=(state_dim,), dtype=np.float32, minimum=act_min, maximum=act_max, name='action') #a_t is an 2darray\n",
    "\n",
    "    \n",
    "        \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "        \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        self._state = self._state + action    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c9068-28cc-41cf-ab83-563b2131cf70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470d9bb-449b-4790-83c6-d1c9f03a90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "ts = env.reset()\n",
    "print(ts)   # should show StepType.FIRST, finite observation & reward=0\n",
    "\n",
    "for _ in range(5):\n",
    "    a = env.action_spec().sample()      # draw a valid random action\n",
    "    ts = env.step(a)\n",
    "    print(ts)  # ensure StepType.MID/LAST, finite obs & reward\n",
    "    if ts.is_last():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fe0a2-3f04-4d69-8106-7a5e35ac7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "timestep = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a70390-a8c6-49b2-a1e6-4656abb7b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f7c9f-f518-4126-85d3-e23d2f983873",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(sub_episode_length + 2):\n",
    "    # sample a random action in the valid range\n",
    "    action = np.random.uniform(act_min, act_max).astype(np.float32)\n",
    "    next_ts = env.step(action)\n",
    "    records.append({\n",
    "        'step': i,\n",
    "        'action': float(action[0]),\n",
    "        'observation': float(next_ts.observation[0]),\n",
    "        'reward': float(next_ts.reward),\n",
    "        'step_type': next_ts.step_type\n",
    "    })\n",
    "    if next_ts.is_last():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79757cf-17ba-45c8-901f-80d9a582aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7aaa7e-6b40-4cd4-8ffa-87cbd23a6446",
   "metadata": {},
   "source": [
    "### End Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497ea58e-6f8f-4cff-bbc4-1239b42e0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 18:28:27,388 - absl - INFO - Spawning all processes.\n",
      "2025-07-01 18:28:27,431 - absl - INFO - Waiting for all processes to start.\n",
      "2025-07-01 18:28:28,077 - absl - INFO - All processes started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_env.batch_size = parallel environment number =  3\n"
     ]
    }
   ],
   "source": [
    "#Create a sequence of parallel environments and batch them, for later use by driver to generate parallel trajectories.\n",
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d3f11f-820d-4585-b5fe-876b10d88785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(8,8,8), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         #discrete_projection_net=_categorical_projection_net\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff8dc87-73c8-49e9-9a6a-b3387f319410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec = train_env.time_step_spec(),\n",
    "        action_spec = train_env.action_spec(),\n",
    "        actor_network = actor_net,\n",
    "        value_network = None,\n",
    "        value_estimation_loss_coef = 0.2,\n",
    "        optimizer = opt,\n",
    "        advantage_fn = None,\n",
    "        use_advantage_loss = False,\n",
    "        gamma = 1.0, #discount factor for future returns\n",
    "        normalize_returns = False, #The instruction says it's better to normalize\n",
    "        gradient_clipping = None,\n",
    "        entropy_regularization = None,\n",
    "        train_step_counter = train_step_counter\n",
    "        )\n",
    "       \n",
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f97fd11-7f9f-4c66-8958-e55eb1c86e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                max_length = episode_length*env_num*2    # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                     env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                     policy = REINFORCE_agent.collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fe475d-a5eb-48f3-884f-11f3d63bda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f3ba2a-d3c2-480a-b418-9ee13a239256",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,update_num):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#Generate Trajectories\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mcollect_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#a batch of trajectories will be saved in replay_buffer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     experience \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39mgather_all() \u001b[38;5;66;03m#get the batch of trajectories, shape=(batch_size, episode_length)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m extract_episode(traj_batch\u001b[38;5;241m=\u001b[39mexperience,epi_length\u001b[38;5;241m=\u001b[39msub_episode_length,attr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#shape=(sub_episode_num, sub_episode_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/drivers/dynamic_episode_driver.py:211\u001b[0m, in \u001b[0;36mDynamicEpisodeDriver.run\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m         time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m         policy_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m         num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m         maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Takes episodes in the environment using the policy and update observers.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m  If `time_step` and `policy_state` are not provided, `run` will reset the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    policy_state: Tensor with final step policy state.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_tf1_allowed()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/drivers/dynamic_episode_driver.py:224\u001b[0m, in \u001b[0;36mDynamicEpisodeDriver._run\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See `run()` docstring for details.\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m   time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m policy_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mget_initial_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_environment.py:209\u001b[0m, in \u001b[0;36mTFEnvironment.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment and returns the current time_step.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        corresponding to `observation_spec()`.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:265\u001b[0m, in \u001b[0;36mTFPyEnvironment._reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(_reset_py)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m   reset_op \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_isolated_reset_py\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No inputs.\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreset_py_func\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies([reset_op]):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time_step()\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:262\u001b[0m, in \u001b[0;36mTFPyEnvironment._reset.<locals>._isolated_reset_py\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_isolated_reset_py\u001b[39m():\n\u001b[0;32m--> 262\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_reset_py\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:211\u001b[0m, in \u001b[0;36mTFPyEnvironment._execute\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    210\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mapply(fn, args\u001b[38;5;241m=\u001b[39margs, kwds\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:259\u001b[0m, in \u001b[0;36mTFPyEnvironment._reset.<locals>._reset_py\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reset_py\u001b[39m():\n\u001b[1;32m    258\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _check_not_called_concurrently(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock):\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py:195\u001b[0m, in \u001b[0;36mPyEnvironment.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ts\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: Subclasses cannot override this directly. Subclasses implement\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        corresponding to `observation_spec()`.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:137\u001b[0m, in \u001b[0;36mParallelPyEnvironment._reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reset all environments and combine the resulting observation.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    Time step with batch dimension.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m   time_steps \u001b[38;5;241m=\u001b[39m [env\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envs]\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking:\n\u001b[1;32m    139\u001b[0m     time_steps \u001b[38;5;241m=\u001b[39m [promise() \u001b[38;5;28;01mfor\u001b[39;00m promise \u001b[38;5;129;01min\u001b[39;00m time_steps]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:137\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reset all environments and combine the resulting observation.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    Time step with batch dimension.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m   time_steps \u001b[38;5;241m=\u001b[39m [\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envs]\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking:\n\u001b[1;32m    139\u001b[0m     time_steps \u001b[38;5;241m=\u001b[39m [promise() \u001b[38;5;28;01mfor\u001b[39;00m promise \u001b[38;5;129;01min\u001b[39;00m time_steps]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:397\u001b[0m, in \u001b[0;36mProcessPyEnvironment.reset\u001b[0;34m(self, blocking)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ts\u001b[38;5;241m.\u001b[39mTimeStep, Promise]:\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reset the environment.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    observation.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m   promise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m blocking:\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m promise()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:341\u001b[0m, in \u001b[0;36mProcessPyEnvironment.call\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously call a method of the external environment.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m  The attribute.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m payload \u001b[38;5;241m=\u001b[39m name, args, kwargs\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "############################################################### - 1st line in Table 2 will be Outputed\n",
    "######## Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = generation_num \n",
    "eval_intv = 100 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('episode of rewards', rewards.round(3))\n",
    "        print('act_std:', actions_distribution.stddev()[0,0]  )\n",
    "        print('act_mean:', actions_distribution.mean()[0,0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print(' ')\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "        \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "     )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f36467ed-be0a-48b2-b4f1-4f2f44985517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c130dd6-afaf-40f4-84bd-659320cbe05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdf33d-d75e-4d3e-95ad-db58fff86f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475251fe-b797-4712-993f-c70a4d97c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
