{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b706a080-294e-40d6-8ff3-d5ac00b5b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:51:46.461196: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-06 21:51:46.462515: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-06 21:51:46.489884: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-06 21:51:46.490606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-06 21:51:47.039474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545560f6-cdc5-4ac1-b46e-9cc95eb00a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import environment\n",
    "from tf_agents.environments import py_environment,utils\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ad6a7f-0d11-40f3-9338-858dc8df796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9282a3c-2c8f-4977-b39d-cdeb0bcb720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.networks.categorical_projection_network import CategoricalProjectionNetwork\n",
    "import os,gc\n",
    "import pygad\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2214ccac-5825-42a4-bdc1-2ac2ccb26f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:53:08.742602: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To limit TensorFlow to a CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7a7b91-2605-4782-9a2a-91855ef3179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Define the objective f to be maximized\n",
    "N = 3   #This the dimension number\n",
    "state_dim = N\n",
    "def f(x,lambda_coef=0.3):\n",
    "    return -(x**2 -1)**2 - lambda_coef*(x-1)**2 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0722d2df-53be-4aa4-8c08-f8a7a8ee3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(f(np.array([1,1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf0407a-3ee1-4cbd-92ad-5c9019525ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 [-1.5 -1.5 -1.5]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "#Set initial x-values for all three algorithms\n",
    "x0_reinforce = np.array([-1.5]*N,dtype=np.float32)\n",
    "sub_episode_length = 50 #number of time_steps in a sub-episode. \n",
    "episode_length = sub_episode_length*6  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 6  #Number of parallel environments, each environment is used to generate an episode\n",
    "print('x0', x0_reinforce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d875eacf-8ad6-4dfe-a29b-60ce4fabb3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sub_episodes used for a single param update: 36\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "#Set hyper-parameters for REINFORCE-OPT\n",
    "generation_num = 100  #number of theta updates for REINFORCE-IP, also serves as the number\n",
    "                      #of generations for GA, and the number of iterations for particle swarm optimization\n",
    "\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9041099-baf2-4a45-9e8d-d11e4cbeb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Schedule = initial_lr * (C/(step+C))\n",
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.0001, C=50000)   \n",
    "opt = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "#opt = tf.keras.optimizers.Adam( )\n",
    "\n",
    "train_step_num = 0\n",
    "\n",
    "\n",
    "act_min = np.array([-1]*N, dtype=np.int32)\n",
    "act_max = np.array([1]*N, dtype=np.int32)\n",
    "step_size = 0.1\n",
    "def compute_reward(x):\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965cd69a-bc30-49eb-88b5-1bda94eb82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Environment\n",
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                            shape=(state_dim,), dtype=np.int32, minimum=0, maximum=act_max-act_min, name='action') #a_t is an 2darray\n",
    "    \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = x0_reinforce\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = x0_reinforce  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "        \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        #Note that we set the action space as a set of non-negative vectors\n",
    "        #action-act_max converts the set to the desired set of negative vectors.\n",
    "        self._state = self._state + (action-act_max)*step_size    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state[0])\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d5b9e-648e-475f-ad2a-a576c0a06ac4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Env test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9562369-a939-4b1b-a79b-5072b6b8a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance\n",
    "init_ts = test_env.reset()\n",
    "x0 = init_ts.observation.numpy()[0]  \n",
    "\n",
    "# construct all “unit” moves in each axis\n",
    "# e.g. if your discrete bins are [-3, -2, …, +3], Δ=±1 in each dim\n",
    "deltas = []\n",
    "for d in range(state_dim):\n",
    "    e = np.zeros_like(x0)\n",
    "    e[d] = 1   # +1 step in dim d\n",
    "    deltas.append(e)\n",
    "    deltas.append(-e)  # also try −1\n",
    "\n",
    "print(\"Flat-line test at x0 =\", x0)\n",
    "for Δ in deltas:\n",
    "    r0 = compute_reward(x0)\n",
    "    r1 = compute_reward(x0 + Δ)\n",
    "    print(f\"  Δ={Δ},  Δr = {r1 - r0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694de102-e958-45b7-9043-46905169c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_action(spec):\n",
    "    \"\"\"Draw one uniform random integer action respecting a BoundedArraySpec.\"\"\"\n",
    "    # spec.minimum and spec.maximum may be scalars or arrays\n",
    "    low = np.array(spec.minimum, dtype=spec.dtype)\n",
    "    high = np.array(spec.maximum, dtype=spec.dtype)\n",
    "    # +1 because np.randint upper bound is exclusive\n",
    "    return np.random.randint(low, high + 1, size=spec.shape, dtype=spec.dtype)\n",
    "\n",
    "def test_env():\n",
    "    np.random.seed(0)\n",
    "    env = Env()\n",
    "\n",
    "    print(\"=== ACTION SPEC ===\")\n",
    "    print(env.action_spec())\n",
    "    print(\"\\n=== OBSERVATION SPEC ===\")\n",
    "    print(env.observation_spec())\n",
    "    print()\n",
    "\n",
    "    # 1) RESET \n",
    "    ts = env.reset()\n",
    "    print(\"RESET ->\")\n",
    "    print(\"  state    =\", ts.observation)\n",
    "    print(\"  reward   =\", ts.reward)\n",
    "    print(\"  stepType =\", ts.step_type)\n",
    "    print()\n",
    "\n",
    "    # 2) STEP RANDOMLY up to sub_episode_length+2\n",
    "    max_steps = 10\n",
    "    for i in range(max_steps):\n",
    "        a = sample_random_action(env.action_spec())\n",
    "        ts = env.step(a)\n",
    "        print(f\"STEP {i+1:2d}\")\n",
    "        print(\"  action   =\", a)\n",
    "        print(\"  new state=\", ts.observation)\n",
    "        print(\"  reward   =\", ts.reward)\n",
    "        print(\"  stepType =\", ts.step_type)\n",
    "        print()\n",
    "        if ts.step_type == StepType.LAST:\n",
    "            print(f\"Episode terminated at step {i+1}.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Did not terminate in {max_steps} steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2704211e-85f8-4e43-8166-b46da96f438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ACTION SPEC ===\n",
      "BoundedArraySpec(shape=(3,), dtype=dtype('int32'), name='action', minimum=0, maximum=[2 2 2])\n",
      "\n",
      "=== OBSERVATION SPEC ===\n",
      "BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n",
      "\n",
      "RESET ->\n",
      "  state    = [-1.5 -1.5 -1.5]\n",
      "  reward   = 0.0\n",
      "  stepType = 0\n",
      "\n",
      "STEP  1\n",
      "  action   = [0 1 0]\n",
      "  new state= [-1.6 -1.5 -1.6]\n",
      "  reward   = 0.5384\n",
      "  stepType = 1\n",
      "\n",
      "STEP  2\n",
      "  action   = [1 1 2]\n",
      "  new state= [-1.6 -1.5 -1.5]\n",
      "  reward   = 0.5384\n",
      "  stepType = 1\n",
      "\n",
      "STEP  3\n",
      "  action   = [0 2 0]\n",
      "  new state= [-1.7 -1.4 -1.6]\n",
      "  reward   = -0.7591\n",
      "  stepType = 1\n",
      "\n",
      "STEP  4\n",
      "  action   = [0 0 2]\n",
      "  new state= [-1.8 -1.5 -1.5]\n",
      "  reward   = -2.3696\n",
      "  stepType = 1\n",
      "\n",
      "STEP  5\n",
      "  action   = [1 2 2]\n",
      "  new state= [-1.8 -1.4 -1.4]\n",
      "  reward   = -2.3696\n",
      "  stepType = 1\n",
      "\n",
      "STEP  6\n",
      "  action   = [0 1 1]\n",
      "  new state= [-1.9 -1.4 -1.4]\n",
      "  reward   = -4.3351\n",
      "  stepType = 1\n",
      "\n",
      "STEP  7\n",
      "  action   = [1 1 0]\n",
      "  new state= [-1.9 -1.4 -1.5]\n",
      "  reward   = -4.3351\n",
      "  stepType = 1\n",
      "\n",
      "STEP  8\n",
      "  action   = [1 0 0]\n",
      "  new state= [-1.9 -1.5 -1.6]\n",
      "  reward   = -4.3351\n",
      "  stepType = 1\n",
      "\n",
      "STEP  9\n",
      "  action   = [1 2 0]\n",
      "  new state= [-1.9 -1.4 -1.7]\n",
      "  reward   = -4.3351\n",
      "  stepType = 1\n",
      "\n",
      "STEP 10\n",
      "  action   = [2 0 1]\n",
      "  new state= [-1.8 -1.5 -1.7]\n",
      "  reward   = -2.3696\n",
      "  stepType = 1\n",
      "\n",
      "Did not terminate in 10 steps.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931feff-af1d-46b1-8c2b-e676122fd0fd",
   "metadata": {},
   "source": [
    "##### End Env test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "885396cc-a679-4097-8dfb-46f10b361241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 11:37:45,930 - absl - INFO - Spawning all processes.\n",
      "2025-07-07 11:37:46,005 - absl - INFO - Waiting for all processes to start.\n",
      "2025-07-07 11:37:46,015 - absl - INFO - All processes started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_env.batch_size = parallel environment number =  6\n"
     ]
    }
   ],
   "source": [
    "#Create a sequence of parallel environments and batch them, for later use by driver to generate parallel trajectories.\n",
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=False) #instance\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aee10c1-5e70-4f6a-995e-60ac8582ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_distribution_network outputs a distribution\n",
    "#it is a neural net which outputs the parameter (mean and sd, named as loc and scale) for a normal distribution\n",
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(10,5), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         discrete_projection_net=CategoricalProjectionNetwork,\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         #continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97537b4-d371-4a1f-a3d3-4d21b56ebe98",
   "metadata": {},
   "source": [
    "#### Examine network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddc15416-23e3-4883-8006-64bd7b44d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " EncodingNetwork (EncodingN  multiple                  95        \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " CategoricalProjectionNetwo  multiple                  54        \n",
      " rk (CategoricalProjectionN                                      \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 149 (596.00 Byte)\n",
      "Trainable params: 149 (596.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "=== projection sub-network ===\n",
      "<tf_agents.networks.categorical_projection_network.CategoricalProjectionNetwork object at 0x7f98e79e2d60>\n",
      "Model: \"CategoricalProjectionNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " logits (Dense)              multiple                  54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54 (216.00 Byte)\n",
      "Trainable params: 54 (216.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "=== trainable variables ===\n",
      "ActorDistributionNetwork/EncodingNetwork/dense/kernel:0 (3, 10)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense/bias:0 (10,)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense_1/kernel:0 (10, 5)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense_1/bias:0 (5,)\n",
      "ActorDistributionNetwork/CategoricalProjectionNetwork/logits/kernel:0 (5, 9)\n",
      "ActorDistributionNetwork/CategoricalProjectionNetwork/logits/bias:0 (9,)\n"
     ]
    }
   ],
   "source": [
    "# High-level Keras summary\n",
    "actor_net.summary()\n",
    "\n",
    "# Dig into the projection sub-network\n",
    "print(\"\\n=== projection sub-network ===\")\n",
    "proj_net = actor_net._projection_networks\n",
    "print(proj_net)\n",
    "# If it’s a Keras Model, you can do:\n",
    "try:\n",
    "    proj_net.summary()\n",
    "except Exception:\n",
    "    # not all TF-Agents networks expose .summary()\n",
    "    print(\"No .summary() on this object; listing its layers instead:\")\n",
    "    for layer in proj_net.layers:\n",
    "        print(\" \", layer.name, layer)\n",
    "\n",
    "# Finally, list all trainable variables so you see exactly which weight matrices live where\n",
    "print(\"\\n=== trainable variables ===\")\n",
    "for v in actor_net.trainable_variables:\n",
    "    print(v.name, v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a402c2a-d9f0-4167-9084-b7fdfd38bfb0",
   "metadata": {},
   "source": [
    "#### End of Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "673d12d5-0289-4a77-8411-cdddd5c7c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec = train_env.time_step_spec(),\n",
    "        action_spec = train_env.action_spec(),\n",
    "        actor_network = actor_net,\n",
    "        value_network = None,\n",
    "        value_estimation_loss_coef = 0.2,\n",
    "        optimizer = opt,\n",
    "        advantage_fn = None,\n",
    "        use_advantage_loss = False,\n",
    "        gamma = 1.0, #discount factor for future returns\n",
    "        normalize_returns = False, #The instruction says it's better to normalize\n",
    "        gradient_clipping = None,\n",
    "        entropy_regularization = None,\n",
    "        train_step_counter = train_step_counter\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e156b-3284-437e-95ae-1acc999a1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bbc80-8ab7-4783-beeb-c679de1431ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                max_length = episode_length*100    # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "#test_buffer is used for evaluating a policy\n",
    "test_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec= REINFORCE_agent.collect_data_spec,  # describe a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size= 1,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                                    # train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "                                                    # batch_size: Batch dimension of tensors when adding to buffer. \n",
    "                max_length = episode_length         # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc980da-47ef-4cac-a1ab-59660fec8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                     env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                     policy = REINFORCE_agent.collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )\n",
    "\n",
    "#For policy evaluation\n",
    "test_driver = py_driver.PyDriver(\n",
    "                                     env = eval_env, #PyEnvironment or TFEnvironment class\n",
    "                                     policy = REINFORCE_agent.policy,\n",
    "                                     observers = [test_buffer.add_batch],\n",
    "                                     max_episodes=1, #optional. If provided, the data generation ends whenever\n",
    "                                                      #either max_steps or max_episodes is reached.\n",
    "                                     max_steps=sub_episode_length\n",
    "                                )\n",
    "\n",
    "#Functions needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83178227-46eb-4965-9523-3796ffb98dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3f7de-dd88-4e56-a7b5-befbccc4c730",
   "metadata": {},
   "source": [
    "#### Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43229ba0-300d-4792-8a20-816ebce446f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = 1\n",
    "eval_intv = 1 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "\n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print('collect_traj:', observations[0,:,0].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:,0]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928fa28-7066-49ed-956f-158f5a133a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations = observations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738ca96-05df-459c-9a9d-fd3dbde04d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0e0eb-d2ea-4f53-be4c-4df892114640",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c78a62-f2cb-4be6-9096-562bbda0fd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5625, 1.5625, 1.5625])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(np.array([-1.5,-1.5,-1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c168dcc-efc3-48a2-8192-58264aca86e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5384, 1.5625, 0.5384])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(np.array([-1.6,-1.5,-1.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6112fcc-2349-4938-b5af-4077fabda639",
   "metadata": {},
   "source": [
    "#### End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4bd4a0-c008-4e34-be82-ea5edda7c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = generation_num \n",
    "eval_intv = 10 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    ######Entropy trace during “burn-in”, Watch the policy’s entropy during the very first few updates\n",
    "    dist = REINFORCE_agent.collect_policy.distribution(time_steps, policy_state=None).action\n",
    "\n",
    "    ent = dist.entropy().numpy()   # shape = (batch_size, state_dim)\n",
    "    print(f\"[step {n:3d}] mean entropy per-dim = {ent.mean():.4f}\")\n",
    "    #######\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if best_step_reward>final_reward:\n",
    "        #print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        #print(\"final reward after udpate:\",final_reward)\n",
    "        #print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print('collect_traj:', observations[0,:,0].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:,0]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max\n",
    "\n",
    "\n",
    "#Compare the trajectory generated by the trained policy,\n",
    "#and the trajectory generated by the gradient descent method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e293f-40bd-42bb-99de-b756f3f00e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851b3c2-7e91-4cce-b77f-2af9da49c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b210e0-c433-44de-9630-740d96f188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b62ee9-9596-4fd4-b5bf-a80cfa8b35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the GD trajectory\n",
    "def forward_fn(x_val):\n",
    "    return -(x_val**2-1)**2 - 0.3*(x_val-1)**2+5\n",
    "\n",
    "def f_grad(x_val):\n",
    "    return -4*(x_val**2-1)*x_val - 0.6*(x_val-1)\n",
    "\n",
    "#compute the function values\n",
    "x_arr = np.linspace(-1.5,1.5,100)\n",
    "f_vals = []\n",
    "for xi in x_arr:\n",
    "    f_vals.append(forward_fn(xi))\n",
    "\n",
    "#compute the gradient descent trajectory\n",
    "initial_x = -1.5\n",
    "#step_size = 0.5\n",
    "grad_trajectory = [[initial_x,forward_fn(initial_x)]]\n",
    "step_num = sub_episode_length\n",
    "current_x = initial_x\n",
    "for stp in range(step_num):\n",
    "    grad = np.sign(f_grad(current_x))*step_size\n",
    "    current_x += grad\n",
    "    grad_trajectory.append([current_x,forward_fn(current_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d119cf-83ed-426e-8cc6-fd98de434805",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## - Figure 1\n",
    "initial_state = [initial_x]\n",
    "env_obj = Env()\n",
    "env_obj.reset()\n",
    "env_obj._state = initial_x\n",
    "REINFORCE_trajectory = [[initial_x, forward_fn(initial_x)]]\n",
    "timestep = ts.TimeStep(step_type=np.array([StepType.MID]), \n",
    "                                reward=np.float32([-forward_fn(initial_x)**2]),  \n",
    "                                discount=np.array(disc_factor,dtype=np.float32), \n",
    "                                observation=np.array([initial_state], dtype=np.float32)\n",
    "                      )\n",
    "for i in range(step_num-1):\n",
    "    act = REINFORCE_agent.collect_policy.action(timestep)[0].numpy()[0]\n",
    "    timestep = env_obj.step(act)\n",
    "    #convert the format of timestep so that agent.policy.action accepts it.\n",
    "    timestep = ts.TimeStep(step_type=np.array([timestep.step_type]), \n",
    "                           reward=np.float32([timestep.reward]), \n",
    "                           discount=np.array(disc_factor,dtype=np.float32), \n",
    "                           observation=np.array([timestep.observation], dtype=np.float32)\n",
    "                          )\n",
    "    REINFORCE_trajectory.append([timestep.observation[0][0],forward_fn(timestep.observation[0][0])])\n",
    "\n",
    "#plot sac trajectory\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.bf'] = 'STIXGeneral:italic:bold'\n",
    "plt.rcParams['mathtext.it'] = 'STIXGeneral:italic'\n",
    "\n",
    "\n",
    "i = 0\n",
    "plt.arrow(x=REINFORCE_trajectory[i][0],\n",
    "          y=REINFORCE_trajectory[i][1],\n",
    "          dx=REINFORCE_trajectory[i+1][0]-REINFORCE_trajectory[i][0],\n",
    "          dy=REINFORCE_trajectory[i+1][1]-REINFORCE_trajectory[i][1],\n",
    "          color='r',width=0.02,label='REINFORCE-OPT')\n",
    "\n",
    "for i in range(0, step_num-2):\n",
    "    plt.arrow(x=REINFORCE_trajectory[i][0],\n",
    "              y=REINFORCE_trajectory[i][1],\n",
    "              dx=REINFORCE_trajectory[i+1][0]-REINFORCE_trajectory[i][0],\n",
    "              dy=REINFORCE_trajectory[i+1][1]-REINFORCE_trajectory[i][1],\n",
    "              color='r',width=0.02)\n",
    "\n",
    "plt.plot(x_arr,f_vals,linestyle='--',label='$\\mathcal{L}(x)=-(x^2-1)^2-0.3(x-1)^2+5$')\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.xlabel('$x$',size=25)\n",
    "plt.ylabel('$\\mathcal{L}(x)$',size=25)\n",
    "plt.legend(loc='lower right',fontsize=20)   \n",
    "plt.savefig(\"escape1D-REINFROCE-traj.eps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plot gd trajectory\n",
    "plt.figure(figsize=(8,6))\n",
    "i = 0\n",
    "plt.arrow(x=grad_trajectory[i][0],\n",
    "          y=grad_trajectory[i][1],\n",
    "          dx=grad_trajectory[i+1][0]-grad_trajectory[i][0],\n",
    "          dy=grad_trajectory[i+1][1]-grad_trajectory[i][1],\n",
    "          color='b',linestyle='--',width=0.02, label='Gradient Ascent')\n",
    "\n",
    "for i in range(0, step_num-1):   \n",
    "    plt.arrow(x=grad_trajectory[i][0],\n",
    "              y=grad_trajectory[i][1],\n",
    "              dx=grad_trajectory[i+1][0]-grad_trajectory[i][0],\n",
    "              dy=grad_trajectory[i+1][1]-grad_trajectory[i][1],\n",
    "              color='b',linestyle='--',width=0.015)\n",
    "\n",
    "plt.plot(x_arr,f_vals,linestyle='--',label='$\\mathcal{L}(x)=-(x^2-1)^2-0.3(x-1)^2+5$')\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.xlabel('$x$',size=25)\n",
    "plt.ylabel('$\\mathcal{L}(x)$',size=25)\n",
    "plt.legend(loc='lower right',fontsize=20)   \n",
    "plt.savefig(\"escape1D-grad-traj.eps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################################ - Figure 2a\n",
    "REINFORCE_fitness_traj = []\n",
    "grad_fitness_traj = []\n",
    "for i in range(step_num):\n",
    "    REINFORCE_fitness_traj.append(REINFORCE_trajectory[i][1])\n",
    "    \n",
    "for i in range(step_num):\n",
    "    grad_fitness_traj.append(grad_trajectory[i][1])\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1,step_num+1),REINFORCE_fitness_traj,color='red',marker='o',linestyle='--',label='REINFORCE-OPT')\n",
    "plt.plot(range(1,step_num+1),grad_fitness_traj,color='blue',marker='x',linestyle='--',label='Gradient Ascent')\n",
    "plt.xlabel('$t$',size=30)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.xlim([0,40]) \n",
    "plt.ylabel('$\\mathcal{L}(x_t)$',size=30)\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.legend(loc='upper left',fontsize=20)\n",
    "#plt.title('$\\mathcal{L}(x_t)$ Trajectory - The 1D Case',size=25)\n",
    "plt.savefig(\"escape-fitness-traj-1D.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991b68d-31a5-4ec3-afb6-7f818b8ec169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba2ecd-d82d-4818-a5d0-9d28dcf3d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip([1,2,3],[-1,-2,-3],[4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532be7d8-75e9-46d9-baf2-1336493e65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69ed0b-7e22-4762-9280-72851296a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2) Sampling vs. greedy policy check\n",
    "##Verify you really are using a stochastic policy for collection, and that it actually samples other bins early on:\n",
    "##If all 10 samples equal greedy (or all equal your initial bin), then you’re accidentally collecting with the deterministic policy or your logits already collapsed.\n",
    "\n",
    "ts = train_env.reset()\n",
    "print(\"Greedy vs. sampled at initial state:\")\n",
    "# greedy “best‐guess” action\n",
    "greedy = REINFORCE_agent.policy.action(ts).action.numpy()\n",
    "print(\"  greedy action =\", greedy)\n",
    "\n",
    "# 10 stochastic samples from collect_policy\n",
    "for i in range(10):\n",
    "    a = REINFORCE_agent.collect_policy.action(ts).action.numpy()\n",
    "    print(f\"  sample {i:2d} →\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89050a0-0f8f-4b91-a68c-8698954338e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
