{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17c319e-b779-4459-a8f7-fb77cfb081c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 10:53:25.644822: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 10:53:25.683148: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-03 10:53:25.986215: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-03 10:53:25.988306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 10:53:26.978037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from custom_normal_projection_network import NormalProjectionNetwork\n",
    "import os,gc\n",
    "import pygad\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfa69ef-ba9a-4f4a-a9ee-60029a76e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 10:53:31.524971: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To limit TensorFlow to a specific set of GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4fb1dc-69b9-43cd-8078-027f25d03eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set initial x-values for all three algorithms\n",
    "N = 4   #This the dimension number\n",
    "state_dim = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe9e73d-29f4-4687-b39a-b4448f4e5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=np.array([-0.5]*N)\n",
    "m2=np.array([0.5]*N)\n",
    "\n",
    "def f(x):\n",
    "    return -np.log(np.sum((x-m1)**2)+0.00001)-np.log(np.sum((x-m2)**2)+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78ae7d4-cb31-4cde-bdb9-4fca19d4d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_multimodal_reward(x):\n",
    "    \"\"\"\n",
    "    x: np.array([x1, x2, x3, x4])\n",
    "    Domains:\n",
    "      x1 ∈ [-3, 5]\n",
    "      x2 ∈ [10, 14]\n",
    "      x3 ∈ [-8, -4]\n",
    "      x4 ∈ [0.5, 1.5]\n",
    "    \"\"\"\n",
    "    ##Only at [1.2, 12, -6, 1]\n",
    "    # Optionally, clip state for extra safety\n",
    "    x1 = np.clip(x[0], -3, 5)\n",
    "    x2 = np.clip(x[1], 10, 14)\n",
    "    x3 = np.clip(x[2], -8, -4)\n",
    "    x4 = np.clip(x[3], 0.5, 1.5)\n",
    "\n",
    "    term1 = (x1-1.2)**2 + 10*np.cos(2*np.pi*(x1-1.2))\n",
    "    term2 = (x2-12)**2 + 8*np.cos(2*np.pi*(x2-12))\n",
    "    term3 = (x3+6)**2 + 6*np.cos(2*np.pi*(x3+6))\n",
    "    term4 = 30*(x4-1)**2 + 5*np.cos(2*np.pi*(x4-1))\n",
    "\n",
    "    # Always positive inside log, and grows slowly for large x\n",
    "    s = 1 + term1 + term2 + term3 + term4\n",
    "\n",
    "    # Prevent log(0) or negative (shouldn't happen with s=1+...)\n",
    "    s = np.abs(s) + 1e-8\n",
    "    reward = -np.log(s)\n",
    "\n",
    "    # Final hard clip (defensive)\n",
    "    reward = np.clip(reward, -100, 100)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5e18b6-d98e-46e0-8357-284fa6ca5a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.401197381995489"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_multimodal_reward([1.2, 12, -6, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0bdb59-5332-4c4f-835b-fc1f1f70bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sub_episodes used for a single param update: 60\n"
     ]
    }
   ],
   "source": [
    "r = np.random.RandomState(0)\n",
    "#x0_reinforce = np.array([0.0]*N)\n",
    "x0_reinforce = np.array([0.0]*N)\n",
    "sub_episode_length = 50 #number of time_steps in a sub-episode. \n",
    "episode_length = sub_episode_length*6  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 10  #Number of parallel environments, each environment is used to generate an episode\n",
    "#pop_size = episode_length*env_num #number of individuals in a generation\n",
    "#ga_init_population = r.uniform(low=-1.0, high=1.0, size=(pop_size,N))\n",
    "#pso_init_population = r.uniform(low=-1.0, high=1.0, size=(pop_size,N))\n",
    "#print('x0', x0_reinforce)\n",
    "#print('Generation size', pop_size)\n",
    "#print('ga_init_pop:',ga_init_population)\n",
    "#print('pso_init_pop:',pso_init_population)\n",
    "\n",
    "\n",
    "################\n",
    "#Set hyper-parameters for REINFORCE-OPT\n",
    "generation_num = 2000  #number of theta updates for REINFORCE-IP, also serves as the number\n",
    "                      #of generations for GA, and the number of iterations for particle swarm optimization\n",
    "\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)\n",
    "\n",
    "train_step_num = 0\n",
    "\n",
    "\n",
    "def compute_reward(x):\n",
    "    return robust_multimodal_reward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b58b24-1c61-4186-950c-25c21beb3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Schedule = initial_lr * (C/(step+C))\n",
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.001, C=50000)   \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "#opt = tf.keras.optimizers.legacy.Adam( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158ee2a2-98af-4426-b8bc-98ce17839800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                            shape=(state_dim,), dtype=np.float32, minimum=np.array([-100.0]*N), maximum=np.array([100.0]*N), name='action') #a_t is an 2darray\n",
    "    \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "        \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        self._state = self._state + action    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196b78ec-e068-45ac-8c9a-9d5d9f1232e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 10:55:29,824 - absl - INFO - Spawning all processes.\n",
      "2025-07-03 10:55:29,915 - absl - INFO - Waiting for all processes to start.\n",
      "2025-07-03 10:55:29,920 - absl - INFO - All processes started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_env.batch_size = parallel environment number =  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n",
      "2025-07-03 11:03:39,279 - absl - ERROR - Error in environment process: Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n",
      "    self._current_time_step = self._step(action)\n",
      "  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n",
      "    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n",
      "    return nest_util.map_structure(\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n",
      "    return _tf_core_map_structure(func, *structure, **kwargs)\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n",
      "    [func(*x) for x in entries],\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n",
      "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
      "  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n",
      "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
      "ValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\n",
      "Got:\n",
      "nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance of parallel environments\n",
    "#Create a sequence of parallel environments and batch them, for later use by driver to generate parallel trajectories.\n",
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2f1d8-3cf8-4d68-a09e-b38d962231cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf1771-54ca-4f8b-8a2b-32279eb3ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_min = np.array([-10.0, -100.0,  -60.0])\n",
    "act_max = np.array([10.0, 100.0, 60.0])\n",
    "class A_Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                            shape=(state_dim,), dtype=np.float32, minimum=act_min, maximum=act_max, name='action') #a_t is an 2darray\n",
    "\n",
    "    \n",
    "        \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if np.isnan(action).any() or np.isinf(action).any():\n",
    "            print(\"Got invalid action at step\", self._step_counter, \":\", action)\n",
    "            raise ValueError(\"Action is NaN or Inf!\")\n",
    "\n",
    "        \n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "        \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        self._state = self._state + action    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        if np.isnan(R) or np.isinf(R):\n",
    "            print(\"Reward is invalid at step\", self._step_counter, \"state=\", self._state)\n",
    "            raise ValueError(\"Reward is NaN or Inf!\")\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d5cbc-507b-4e8d-a0a7-10bbe8d45a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(A_Env(), check_dims=True) #instance of parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097915c7-adb0-400b-9923-a2e82f11e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_distribution_network outputs a distribution\n",
    "#it is a neural net which outputs the parameter (mean and sd, named as loc and scale) for a normal distribution\n",
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(8,8,8), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         #discrete_projection_net=_categorical_projection_net\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )\n",
    "\n",
    "\n",
    "\n",
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec = train_env.time_step_spec(),\n",
    "        action_spec = train_env.action_spec(),\n",
    "        actor_network = actor_net,\n",
    "        value_network = None,\n",
    "        value_estimation_loss_coef = 0.2,\n",
    "        optimizer = opt,\n",
    "        advantage_fn = None,\n",
    "        use_advantage_loss = False,\n",
    "        gamma = 1.0, #discount factor for future returns\n",
    "        normalize_returns = False, #The instruction says it's better to normalize\n",
    "        gradient_clipping = None,\n",
    "        entropy_regularization = None,\n",
    "        train_step_counter = train_step_counter\n",
    "        )\n",
    "       \n",
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fc3d9-3f5d-4b9c-8e6f-ac473591c5af",
   "metadata": {},
   "source": [
    "### End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d5b8cb-60f8-4a86-8ab9-79260d854173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_distribution_network outputs a distribution\n",
    "#it is a neural net which outputs the parameter (mean and sd, named as loc and scale) for a normal distribution\n",
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(16,16,16,16), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         #discrete_projection_net=_categorical_projection_net\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )\n",
    "\n",
    "\n",
    "\n",
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec = train_env.time_step_spec(),\n",
    "        action_spec = train_env.action_spec(),\n",
    "        actor_network = actor_net,\n",
    "        value_network = None,\n",
    "        value_estimation_loss_coef = 0.2,\n",
    "        optimizer = opt,\n",
    "        advantage_fn = None,\n",
    "        use_advantage_loss = False,\n",
    "        gamma = 1.0, #discount factor for future returns\n",
    "        normalize_returns = False, #The instruction says it's better to normalize\n",
    "        gradient_clipping = None,\n",
    "        entropy_regularization = None,\n",
    "        train_step_counter = train_step_counter\n",
    "        )\n",
    "       \n",
    "REINFORCE_agent.initialize()\n",
    "\n",
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                max_length = episode_length*100    # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                        env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                        policy = REINFORCE_agent.collect_policy,\n",
    "                                        observers = [replay_buffer.add_batch],\n",
    "                                        num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1c2d89-7708-4771-91e6-753957fb5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652773f1-fcc8-4bbc-a656-f5bd99b2678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1179/3742384916.py:14: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 10:55:42,686 - tensorflow - WARNING - From /tmp/ipykernel_1179/3742384916.py:14: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step no.= 1\n",
      "best_solution of this generation= [-0.23737875 10.51534    -0.06293088 -3.072576  ]\n",
      "best step reward= 0.559 0.5594446614125345\n",
      "avg step reward= -3.445\n",
      "act_std: tf.Tensor([0.9740769 0.9740769 0.9740769 0.9740769], shape=(4,), dtype=float32)\n",
      "act_mean: tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n",
      "best_step_index: [18, 30]\n",
      " \n",
      "final reward before udpate: -1000\n",
      "final reward after udpate: 0.55944467\n",
      "updated final_solution= [-0.23737875 10.51534    -0.06293088 -3.072576  ]\n",
      "final reward before udpate: 0.55944467\n",
      "final reward after udpate: 0.7997816\n",
      "updated final_solution= [-0.29533195 11.402552   -2.0475712  -4.555007  ]\n",
      "final reward before udpate: 0.7997816\n",
      "final reward after udpate: 1.2904264\n",
      "updated final_solution= [ 1.610987  13.416494  -2.713225  -2.4524508]\n",
      "final reward before udpate: 1.2904264\n",
      "final reward after udpate: 4.7207007\n",
      "updated final_solution= [ 2.9500306 13.412858  -5.309688  41.184307 ]\n",
      "train_step no.= 101\n",
      "best_solution of this generation= [  4.717666  31.766302  -6.462462 -27.28265 ]\n",
      "best step reward= -2.511 -2.510858887676248\n",
      "avg step reward= -3.758\n",
      "act_std: tf.Tensor([1.0409822  0.38539237 1.9785789  2.1931026 ], shape=(4,), dtype=float32)\n",
      "act_mean: tf.Tensor([  6.443561   31.764734   -7.7090616 -26.229322 ], shape=(4,), dtype=float32)\n",
      "best_step_index: [8, 1]\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/tf_agents/utils/common.py:1443: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return state is not None and state is not () and state is not []\n",
      "/root/.local/lib/python3.8/site-packages/tf_agents/utils/common.py:1443: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return state is not None and state is not () and state is not []\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Traceback (most recent call last):\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n    result = getattr(env, name)(*args, **kwargs)\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n    self._current_time_step = self._step(action)\n  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n    return nest_util.map_structure(\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n    return _tf_core_map_structure(func, *structure, **kwargs)\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n    [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n    [func(*x) for x in entries],\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n    raise ValueError('Received a time_step input that converted to a nan array.'\nValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\nGot:\nnan\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,update_num):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#Generate Trajectories\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mcollect_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#a batch of trajectories will be saved in replay_buffer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     experience \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39mgather_all() \u001b[38;5;66;03m#get the batch of trajectories, shape=(batch_size, episode_length)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m extract_episode(traj_batch\u001b[38;5;241m=\u001b[39mexperience,epi_length\u001b[38;5;241m=\u001b[39msub_episode_length,attr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#shape=(sub_episode_num, sub_episode_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/drivers/dynamic_episode_driver.py:211\u001b[0m, in \u001b[0;36mDynamicEpisodeDriver.run\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m         time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m         policy_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m         num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m         maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Takes episodes in the environment using the policy and update observers.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m  If `time_step` and `policy_state` are not provided, `run` will reset the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    policy_state: Tensor with final step policy state.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_tf1_allowed()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/drivers/dynamic_episode_driver.py:238\u001b[0m, in \u001b[0;36mDynamicEpisodeDriver._run\u001b[0;34m(self, time_step, policy_state, num_episodes, maximum_iterations)\u001b[0m\n\u001b[1;32m    233\u001b[0m counter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros(batch_dims, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    235\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m num_episodes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_episodes\n\u001b[1;32m    236\u001b[0m [_, time_step, policy_state] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    237\u001b[0m     tf\u001b[38;5;241m.\u001b[39mstop_gradient,\n\u001b[0;32m--> 238\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_condition_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_body_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdriver_loop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time_step, policy_state\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:648\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    641\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    647\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/ops/while_loop.py:252\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/ops/while_loop.py:499\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    496\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    497\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 499\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    501\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/drivers/dynamic_episode_driver.py:141\u001b[0m, in \u001b[0;36mDynamicEpisodeDriver._loop_body_fn.<locals>.loop_body\u001b[0;34m(counter, time_step, policy_state)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# TODO(b/134487572): TF2 while_loop seems to either ignore\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# parallel_iterations or doesn't properly propagate control dependencies\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# from one step to the next. Without this dep, self.env.step() is called\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten([time_step])):\n\u001b[0;32m--> 141\u001b[0m   next_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m policy_state \u001b[38;5;241m=\u001b[39m action_step\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_bandit_env:\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;66;03m# For Bandits we create episodes of length 1.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;66;03m# Since the `next_time_step` is always of type LAST we need to replace\u001b[39;00m\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;66;03m# the step type of the current `time_step` to FIRST.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_environment.py:241\u001b[0m, in \u001b[0;36mTFEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Steps the environment according to the action.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m  If the environment returned a `TimeStep` with `StepType.LAST` at the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m        corresponding to `observation_spec()`.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:315\u001b[0m, in \u001b[0;36mTFPyEnvironment._step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (action\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         (dim_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim_value \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)):\n\u001b[1;32m    311\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected actions whose major dimension is batch_size (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    313\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut saw action with shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    314\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, action\u001b[38;5;241m.\u001b[39mshape, action))\n\u001b[0;32m--> 315\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_isolated_step_py\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_step_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep_py_func\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_from_numpy_function_outputs(outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:302\u001b[0m, in \u001b[0;36mTFPyEnvironment._step.<locals>._isolated_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_isolated_step_py\u001b[39m(\u001b[38;5;241m*\u001b[39mflattened_actions):\n\u001b[0;32m--> 302\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_step_py\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflattened_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:211\u001b[0m, in \u001b[0;36mTFPyEnvironment._execute\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    210\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mapply(fn, args\u001b[38;5;241m=\u001b[39margs, kwds\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py:298\u001b[0m, in \u001b[0;36mTFPyEnvironment._step.<locals>._step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _check_not_called_concurrently(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock):\n\u001b[1;32m    296\u001b[0m   packed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m    297\u001b[0m       structure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_spec(), flat_sequence\u001b[38;5;241m=\u001b[39mflattened_actions)\n\u001b[0;32m--> 298\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mstep(packed)\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py:232\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step)):\n\u001b[1;32m    230\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:159\u001b[0m, in \u001b[0;36mParallelPyEnvironment._step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# When blocking is False we get promises that need to be called.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking:\n\u001b[0;32m--> 159\u001b[0m   time_steps \u001b[38;5;241m=\u001b[39m [promise() \u001b[38;5;28;01mfor\u001b[39;00m promise \u001b[38;5;129;01min\u001b[39;00m time_steps]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack_time_steps(time_steps)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:159\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# When blocking is False we get promises that need to be called.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking:\n\u001b[0;32m--> 159\u001b[0m   time_steps \u001b[38;5;241m=\u001b[39m [\u001b[43mpromise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m promise \u001b[38;5;129;01min\u001b[39;00m time_steps]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack_time_steps(time_steps)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py:442\u001b[0m, in \u001b[0;36mProcessPyEnvironment._receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_EXCEPTION:\n\u001b[1;32m    441\u001b[0m   stacktrace \u001b[38;5;241m=\u001b[39m payload\n\u001b[0;32m--> 442\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(stacktrace)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_RESULT:\n\u001b[1;32m    444\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
      "\u001b[0;31mException\u001b[0m: Traceback (most recent call last):\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/parallel_py_environment.py\", line 478, in _worker\n    result = getattr(env, name)(*args, **kwargs)\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 232, in step\n    self._current_time_step = self._step(action)\n  File \"/tmp/ipykernel_1179/3468414657.py\", line 93, in _step\n    return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in transition\n    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 624, in map_structure\n    return nest_util.map_structure(\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1054, in map_structure\n    return _tf_core_map_structure(func, *structure, **kwargs)\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in _tf_core_map_structure\n    [func(*x) for x in entries],\n  File \"/root/anaconda3/envs/myrf/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py\", line 1094, in <listcomp>\n    [func(*x) for x in entries],\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 225, in <lambda>\n    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n  File \"/root/.local/lib/python3.8/site-packages/tf_agents/trajectories/time_step.py\", line 38, in _as_array\n    raise ValueError('Received a time_step input that converted to a nan array.'\nValueError: Received a time_step input that converted to a nan array. Did you accidentally set some input value to None?.\nGot:\nnan\n"
     ]
    }
   ],
   "source": [
    "############################################################### - 1st line in Table 2 will be Outputed\n",
    "######## Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = generation_num \n",
    "eval_intv = 100 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),robust_multimodal_reward(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('episode of rewards', rewards.round(3))\n",
    "        print('act_std:', actions_distribution.stddev()[0,0]  )\n",
    "        print('act_mean:', actions_distribution.mean()[0,0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print(' ')\n",
    "    \n",
    "    if best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "        \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "     )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4606a-4b97-4b35-8a6f-05ef73ba1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cefd7-88f0-4146-968b-b20b52229fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cab8c8-a2a7-4a9b-989d-c0e830244bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
