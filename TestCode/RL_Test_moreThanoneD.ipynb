{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd339c7-a00c-45ca-83eb-eb9c89073d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 14:11:01.376512: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-09 14:11:01.400350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-09 14:11:01.400376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-09 14:11:01.400962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-09 14:11:01.406223: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 14:11:01.887539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec,tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.networks.categorical_projection_network import CategoricalProjectionNetwork\n",
    "#from custom_normal_projection_network import NormalProjectionNetwork\n",
    "import os,gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78f0ab1-c481-4dd3-a1d2-5663a32530e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#To limit TensorFlow to CPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m cpus \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      4\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mset_visible_devices(cpus[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#To limit TensorFlow to CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5763e9-705e-4314-addc-7e1064cab3a0",
   "metadata": {},
   "source": [
    "#### Comment 1\n",
    "In this way, set the reward function. it also the function for further test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32a925-026f-467d-b580-3047e41fa8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Define the objective f to be maximized\n",
    "N = 3   #This the dimension number\n",
    "state_dim = N\n",
    "def f(x,lambda_coef=0.3):\n",
    "    return np.sum(-(x**2 -1)**2 - lambda_coef*(x-1)**2 + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bdcf6-9067-4d1e-b63a-66932f6aacfa",
   "metadata": {},
   "source": [
    "#### Comment 2\n",
    "In this way, setting the starting value as x0_reinforce\n",
    "sub_episode is the serial in a monte carlo model, so the mento carlo is running six time in each generation.\n",
    "in total \n",
    "sub_episode_length * sub_episode_num steps\n",
    "\n",
    "althoug the episode num need consider the parallel enviroment num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6af4c2-c54e-4ec4-8015-d691ce255221",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#Set initial x-values for all three algorithms\n",
    "x0_reinforce = np.array([-1.5]*N,dtype=np.float32)\n",
    "final_reward   = -1e9\n",
    "final_solution = x0_reinforce.copy()    # so it always exists\n",
    "\n",
    "sub_episode_length = 30 #number of time_steps in a sub-episode. \n",
    "episode_length = sub_episode_length*10  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 6  #Number of parallel environments, each environment is used to generate an episode\n",
    "print('x0', x0_reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe348d-436a-4c0c-98d3-ff84422f6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_num = 100  #number of theta updates for REINFORCE-IP\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55af3-7385-45ba-98cf-9ed4df202743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.00001, C=100000)   \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dc8f1-4480-4596-80c9-dfcb8f4c5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_min = np.array([-1]*N, dtype=np.int32)\n",
    "act_max = np.array([1]*N, dtype=np.int32)\n",
    "#x0_reinforce = act_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b1acc-64b0-4670-abed-c1c216b3194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_num = 0\n",
    "step_size = np.array([0.1, 0.1, 0.1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cd40c-bfec-416e-9253-e46e144710a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(x):\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f308304-5655-41ef-9e69-89cac8442630",
   "metadata": {},
   "source": [
    "#### Comment 3\n",
    "This part we have a key factor that distinguish to main project. use the MultiCategoricalProjectionNetwork\n",
    "to support the action here, in the origin project, the CategoricalProjectionNetwork only able to support the action dimensions\n",
    "that supports the same action space. This is not our case, we have tha action, require the different action space.\n",
    "There are three method to solve this,<br/>\n",
    "1, simply flat it to one array(flat_array.txt). https://github.com/tensorflow/agents/issues/702, but may account parameters explorion.<br/>\n",
    "2, use muti-head MultiCategoricalProjectionNetwork. https://gist.github.com/sidney-tio/66abada949f1b629dd9ee28777d402d5<br/>\n",
    "3, turing into the continous space<br/>\n",
    "\n",
    "The method might become parameters exploration if action space go so high, therefore here, we try the method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91475c0d-dc1c-408d-ad3e-40b5dba4261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multicategorical_projection import MultiCategoricalProjectionNetwork\n",
    "#from MultiCategoricalProjectionNetwork_T import MultiCategoricalProjectionNetwork_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fa91b-ef15-483d-a6b6-d6abbe722ab1",
   "metadata": {},
   "source": [
    "#### Comment 4\n",
    "It worth dive into the real update process in the step function below.<br/>\n",
    "The update is according to the equation: <br/>\n",
    "self._state = self._state + (action-act_max)*step_size <br/>\n",
    "then using the state calcualted the reward. <br/>\n",
    "R = compute_reward(self._state) <br/>\n",
    "The thought behind the behaviour is due to the directly use the action as state observation might create unstable duing to the spare samping, thus, instead replace the state with the change direction.\n",
    "after readjust the action space, act_max will work as the middle value to judge it is the postive or negative step size in next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fef87f-f4a8-4b3d-a9a9-fe54a442a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        #self._action_spec = array_spec.BoundedArraySpec(\n",
    "        #                    shape=(state_dim,), dtype=np.int32, \n",
    "        #                    minimum=np.array([0]*N), \n",
    "        #                    maximum=act_max-act_min, \n",
    "        #                    name='action') #a_t is an 2darray\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                                shape=(state_dim,),\n",
    "                                dtype=np.int32,\n",
    "                                minimum=np.array([0]*N),\n",
    "                                maximum=act_max-act_min,\n",
    "                                name='action')\n",
    "    \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "     \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        #Note that we set the action space as a set of non-negative vectors\n",
    "        #action-act_max converts the set to the desired set of negative vectors.\n",
    "\n",
    "        self._state = self._state + (action-act_max)*step_size    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fa1f1-3147-4747-947a-d3117e1296b6",
   "metadata": {},
   "source": [
    "##### for ENV test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c95f2-614a-45bf-8635-a98bdee15fe0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### test1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfb09c-a12b-444e-b609-1f2eced8403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f382a8a-733e-423e-9a4b-74e929379a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = test_env.reset()\n",
    "x0 = init_ts.observation.numpy()[0]  \n",
    "\n",
    "# construct all “unit” moves in each axis\n",
    "# e.g. if your discrete bins are [-3, -2, …, +3], Δ=±1 in each dim\n",
    "deltas = []\n",
    "for d in range(state_dim):\n",
    "    e = np.zeros_like(x0)\n",
    "    e[d] = 1   # +1 step in dim d\n",
    "    deltas.append(e)\n",
    "    deltas.append(-e)  # also try −1\n",
    "\n",
    "print(\"Flat-line test at x0 =\", x0)\n",
    "for Δ in deltas:\n",
    "    r0 = compute_reward(x0)\n",
    "    r1 = compute_reward(x0 + Δ)\n",
    "    print(f\"  Δ={Δ},  Δr = {r1 - r0:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745af216-1e69-49f5-82ff-0c988fbf421c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### test2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc64f9-89b7-428b-821b-458bddf9e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_action(spec):\n",
    "    \"\"\"Draw one uniform random integer action respecting a BoundedArraySpec.\"\"\"\n",
    "    # spec.minimum and spec.maximum may be scalars or arrays\n",
    "    low = np.array(spec.minimum, dtype=spec.dtype)\n",
    "    high = np.array(spec.maximum, dtype=spec.dtype)\n",
    "    # +1 because np.randint upper bound is exclusive\n",
    "    return np.random.randint(low, high + 1, size=spec.shape, dtype=spec.dtype)\n",
    "\n",
    "def test_env():\n",
    "    np.random.seed(0)\n",
    "    env = Env()\n",
    "\n",
    "    print(\"=== ACTION SPEC ===\")\n",
    "    print(env.action_spec())\n",
    "    print(\"\\n=== OBSERVATION SPEC ===\")\n",
    "    print(env.observation_spec())\n",
    "    print()\n",
    "\n",
    "    # 1) RESET \n",
    "    ts = env.reset()\n",
    "    print(\"RESET ->\")\n",
    "    print(\"  state    =\", ts.observation)\n",
    "    print(\"  reward   =\", ts.reward)\n",
    "    print(\"  stepType =\", ts.step_type)\n",
    "    print()\n",
    "\n",
    "    # 2) STEP RANDOMLY up to sub_episode_length+2\n",
    "    max_steps = 10\n",
    "    for i in range(max_steps):\n",
    "        a = sample_random_action(env.action_spec())\n",
    "        ts = env.step(a)\n",
    "        print(f\"STEP {i+1:2d}\")\n",
    "        print(\"  action   =\", a)\n",
    "        print(\"  new state=\", ts.observation)\n",
    "        print(\"  reward   =\", ts.reward)\n",
    "        print(\"  stepType =\", ts.step_type)\n",
    "        print()\n",
    "        if ts.step_type == StepType.LAST:\n",
    "            print(f\"Episode terminated at step {i+1}.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Did not terminate in {max_steps} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec40b54-fe99-471a-bb7a-df4502ecacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f664d7b-2827-4680-8c8a-c04e09e420b8",
   "metadata": {},
   "source": [
    "##### Env test END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff928bb-2989-4295-a268-ca83d26b90c0",
   "metadata": {},
   "source": [
    "#### Comment 5\n",
    "train and eval happen in the same iteration, eval is like a examination on the current train result, in this way got the extra debugging process to look at the chaos come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49cf54-0bfd-4637-9d69-4e4f3d3b3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=False) #instance\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)\n",
    "\n",
    "###Without the Parallel\n",
    "#train_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance of parallel environments\n",
    "#eval_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=False) #instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c531ed-7e45-4d7c-9df2-1d26bdd574f6",
   "metadata": {},
   "source": [
    "#### comment 6 How to set network\n",
    "On the how to set the neural network in setting.\n",
    "The neural network is create the following structure use the code for actor_net\n",
    "\n",
    "Input → Dense(10) → Activation(tanh) → Dense(5) → Activation(tanh) → [projection head]\n",
    "\n",
    "Input is the observation_spec, then the projection head is the action_spec. Unlike the supervised learning process, it trains with the forward and back prop with the loop. The reinforcement actually like a delay update, decouple the data collection(mento-calo process) and gradient update(using loss function to gradient update), the reward of vanilla reinforcement(pure Monte Carlo without bootstraping or n-step return these kind of policy optimal algorithm) only sum up in the end of the episode.\n",
    "\n",
    "in the discrete_projection_net, persumming we put a muti-dimesion predict layer here to predict the action, the place it put is after the Dense(5) layer above it send to this special neural layer.\n",
    "\n",
    "--Dense(5)-->   (batch,  5)  --tanh--> CategoricalNetwork -> projection head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c92b-7c31-4ce2-9619-b530ca687970",
   "metadata": {},
   "source": [
    "#### comment 7 CategoricalProjectionNetwork and MultiCategoricalProjectionNetwork\n",
    "The general idea of this Categorical network is to build the layer, that mapping to the all action point to the logits(the neural on the network to predicts the value), which suppose to be wired up in the policy network. The higher policy should able to do two seperate output, a log probability and a action sampler. \n",
    "\n",
    "The init() and call() function all invoke by the higher policy netwrok, like ActorDistributionNetwork here it's leave the set on the final layer it applied \"discrete_projection_net=\" before the projection head. actually the class CategoricalProjectionNetwork(network.DistributionNetwork), it use the inheritance here, then later it will reinvoke the paranet method by super(CategoricalProjectionNetwork, self).__init__. The actual init() should happened when intitial do the Network build ActorDistributionNetwork(...), although the call() function is invoked when the policy is on working scenario. The call() is warped up in the driver and the agent, in the <br/>\n",
    "action_step = collect_policy.action(time_step, policy_state)\n",
    "\n",
    "The main different is on the multi-head one, because the dimension are different, they need extra split on the netwrok. The detail can see in the _output_distribution_spec in the multi-head situation, it runs with the further MultiCategoricalDistributionBlock, use to spilt more sub-logits, according to the structure. Also there is the extra components in call function, it related to the mask(some special scenario to create the condtions), I believe this project do not have this feature here.\n",
    "\n",
    "extra: The observation and rewards, also with the different, consider the example in original reward, def f(x,lambda_coef=0.3):return -(x**2 -1)**2 - lambda_coef*(x-1)**2 + 5, actually able to return the as many dimension of the reward as the input action, in contrast, the scenario we want use the netwrok to reture a single reward value. like with np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00653e70-35ff-4777-952d-841d6b9e4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_distribution_network outputs a distribution\n",
    "#it is a neural net which outputs the parameter (mean and sd, named as loc and scale) for a normal distribution\n",
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(10,5), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         discrete_projection_net=MultiCategoricalProjectionNetwork,\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         #continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914728b8-8880-4695-a9bf-0ddf50eeb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "#        time_step_spec = train_env.time_step_spec(),\n",
    "#        action_spec = train_env.action_spec(),\n",
    "#        actor_network = actor_net,\n",
    "#        value_network = None,\n",
    "#        value_estimation_loss_coef = 0.2,\n",
    "#        optimizer = opt,\n",
    "#        advantage_fn = None,\n",
    "#        use_advantage_loss = False,\n",
    "#        gamma = 1.0, #discount factor for future returns\n",
    "#        normalize_returns = True, #The instruction says it's better to normalize\n",
    "#        gradient_clipping = 1.0,\n",
    "#        entropy_regularization = 0.05,\n",
    "#        train_step_counter = train_step_counter\n",
    "#        )\n",
    "\n",
    "\n",
    "#Original agent\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "       time_step_spec = train_env.time_step_spec(),\n",
    "       action_spec = train_env.action_spec(),\n",
    "       actor_network = actor_net,\n",
    "       value_network = None,\n",
    "       value_estimation_loss_coef = 0.2,\n",
    "       optimizer = opt,\n",
    "       advantage_fn = None,\n",
    "       use_advantage_loss = False,\n",
    "       gamma = 1.0, #discount factor for future returns\n",
    "       normalize_returns = False, #The instruction says it's better to normalize\n",
    "       gradient_clipping = None,\n",
    "       entropy_regularization = 0.01,\n",
    "       train_step_counter = train_step_counter\n",
    "       )\n",
    "\n",
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fded647-cc32-4588-a97b-e89a5e861ab3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Examine the actor net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fc05d-8bf0-47f2-9ab8-9776b1cb373d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# High-level Keras summary\n",
    "actor_net.summary()\n",
    "\n",
    "# Dig into the projection sub-network\n",
    "print(\"\\n=== projection sub-network ===\")\n",
    "proj_net = actor_net._projection_networks\n",
    "print(proj_net)\n",
    "# If it’s a Keras Model, you can do:\n",
    "try:\n",
    "    proj_net.summary()\n",
    "except Exception:\n",
    "    # not all TF-Agents networks expose .summary()\n",
    "    print(\"No .summary() on this object; listing its layers instead:\")\n",
    "    for layer in proj_net.layers:\n",
    "        print(\" \", layer.name, layer)\n",
    "\n",
    "# Finally, list all trainable variables so you see exactly which weight matrices live where\n",
    "print(\"\\n=== trainable variables ===\")\n",
    "for v in actor_net.trainable_variables:\n",
    "    print(v.name, v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3866a-731d-41be-a462-1798b3f765ff",
   "metadata": {},
   "source": [
    "##### end of examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581cba1-fbe5-4c65-9ca6-a292047efbe1",
   "metadata": {},
   "source": [
    "#### Comment 7 The max_length has optimised\n",
    "Left the episode_length*env_num*2 rather than episode_length*100 should large enough for the headroom.\n",
    "reply buffer use in the trainning procedure, and clean it self in each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc354fb-5e6c-4921-b16e-70cf26016bce",
   "metadata": {},
   "source": [
    "##### Agent Policy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f22ff-ce6f-4959-894b-5619634dee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2) Sampling vs. greedy policy check\n",
    "##Verify you really are using a stochastic policy for collection, and that it actually samples other bins early on:\n",
    "##If all 10 samples equal greedy (or all equal your initial bin), then you’re accidentally collecting with the deterministic policy or your logits already collapsed.\n",
    "\n",
    "ts = train_env.reset()\n",
    "print(\"Greedy vs. sampled at initial state:\")\n",
    "# greedy “best‐guess” action\n",
    "greedy = REINFORCE_agent.policy.action(ts).action.numpy()\n",
    "print(\"  greedy action =\", greedy)\n",
    "\n",
    "# 10 stochastic samples from collect_policy\n",
    "for i in range(10):\n",
    "    a = REINFORCE_agent.collect_policy.action(ts).action.numpy()\n",
    "    print(f\"  sample {i:2d} →\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffceab0f-c839-4aeb-b3c5-70fc05b21708",
   "metadata": {},
   "source": [
    "##### End Policy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45605d6-b5c2-4b92-b9ae-ed5575377489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                \n",
    "                max_length = 100    \n",
    "    \n",
    "                # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "#test_buffer is used for evaluating a policy\n",
    "test_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec= REINFORCE_agent.collect_data_spec,  # describe a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size= 1,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                                    # train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "                                                    # batch_size: Batch dimension of tensors when adding to buffer. \n",
    "                max_length = episode_length         # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c050cc-7da9-4fd3-b0d3-35cdce6b36f0",
   "metadata": {},
   "source": [
    "#### Comment 8\n",
    "In which way to select a proper policy. The chosen on the driver is not the main concern. It is mainly how you want to trainning in the which steps level. DynamicEpisodeDriver is more suitabale for trainning in the parallel env, pydriver is sutiable to the single episode, there are other drivers existing such as,  DynamicStepDriver(able to control the exactly how many steps want to run), onstepdriver(only run one steps). \n",
    "\n",
    "really distinguish the trainning and evaluation is on how to set the policy dependence.<br/>\n",
    "policy = REINFORCE_agent.collect_policy, stochastic policy<br/>\n",
    "policy = REINFORCE_agent.policy,greedy policy<br/>\n",
    "\n",
    "generally, the trainning is on the stochastic policy collection, but the evaluation is on the greedy policy picks arg⁡max⁡𝑎 𝜋𝜃(𝑎∣𝑠)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75222c56-ee74-4935-9fba-e07cbbb2fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                     env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                     policy = REINFORCE_agent.collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )\n",
    "\n",
    "#For policy evaluation\n",
    "test_driver = py_driver.PyDriver(\n",
    "                                     env = eval_env, #PyEnvironment or TFEnvironment class\n",
    "                                     policy = REINFORCE_agent.policy,\n",
    "                                     observers = [test_buffer.add_batch],\n",
    "                                     max_episodes=1, #optional. If provided, the data generation ends whenever\n",
    "                                                      #either max_steps or max_episodes is reached.\n",
    "                                     max_steps=sub_episode_length\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd7295-a888-459c-aec7-dc1a70361ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e75fb-5fb8-4e5d-9c4c-420f4dbf4f84",
   "metadata": {},
   "source": [
    "##### Test Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547ccad-6318-49db-99ef-0341e859d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = 3\n",
    "eval_intv = 1 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "\n",
    "\n",
    "        ########################Check the action distribution\n",
    "        #######\n",
    "        logits = actions_distribution.parameters[\"logits\"]\n",
    "        \n",
    "        print(\"min / max / mean logits =\",\n",
    "              tf.reduce_min(logits).numpy(),\n",
    "              tf.reduce_max(logits).numpy(),\n",
    "              tf.reduce_mean(logits).numpy())\n",
    "        #######\n",
    "        ########################\n",
    "        \n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "\n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        #print('collect_traj:', observations[0,:,0].numpy())\n",
    "        ##  I persume this means the a random trajectories give in the scenario\n",
    "        print('collect_traj:', observations[0,:].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcabb5c-3765-4f79-8e39-f82b0d16d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4a3d4-ec90-4675-a9b6-7aa45a812fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations = observations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc8d23-f76b-4324-9efc-cb7bb7324a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7403109-51a5-4189-9aa6-97ad71abe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90587243-b892-4558-a702-22eff75bea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bde81-dc24-4be5-998d-b2b26b854526",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(np.array([-2.6,-2.3,-2.7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c1324-68db-4425-9055-fd27c33c0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 1, 1]-act_max)*step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fbe62-4d5f-469e-aaf9-42446981683e",
   "metadata": {},
   "source": [
    "##### end of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ed32e-eb82-4862-9572-c2acfee81263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d5655-3b22-4ed2-9df5-bbd9088389f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = 500 \n",
    "eval_intv = 30 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "#final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "\n",
    "        ######Entropy trace during “burn-in”, Watch the policy’s entropy during the very first few updates\n",
    "\n",
    "        ent = actions_distribution.entropy().numpy()   # shape = (batch_size, state_dim)\n",
    "        print(f\"[step {n:3d}] mean entropy per-dim = {ent.mean():.4f}\")\n",
    "        #######\n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print('collect_traj:', observations[0,:].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874114e-b14c-49e2-98c7-ffa88292dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(np.array([-1.5,-1.5,-1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db9a8-3b23-4c17-a771-0141e4d94fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8e7cb-7dc6-46b0-a2a1-f58f3437a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9d488-1fbd-45d7-8b95-c8d0eb9cd07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d038c-3404-4bd7-81b2-cc21da86dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,10],[3,4,13],[1,4,7],[1,9,102]]\n",
    "a = np.array(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d39e3-65d1-4782-88e2-58db29b28d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b892e-1fd7-43b6-878c-58a94929eb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
