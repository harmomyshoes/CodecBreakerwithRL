{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd339c7-a00c-45ca-83eb-eb9c89073d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 12:28:42.027958: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-07 12:28:42.029961: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 12:28:42.065303: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 12:28:42.066439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-07 12:28:42.661075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec,tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.networks.categorical_projection_network import CategoricalProjectionNetwork\n",
    "#from custom_normal_projection_network import NormalProjectionNetwork\n",
    "import os,gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78f0ab1-c481-4dd3-a1d2-5663a32530e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 12:28:52.177692: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To limit TensorFlow to CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5763e9-705e-4314-addc-7e1064cab3a0",
   "metadata": {},
   "source": [
    "#### Comment 1\n",
    "In this way, set the reward function. it also the function for further test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a32a925-026f-467d-b580-3047e41fa8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Define the objective f to be maximized\n",
    "N = 3   #This the dimension number\n",
    "state_dim = N\n",
    "def f(x,lambda_coef=0.3):\n",
    "    return np.sum(-(x**2 -1)**2 - lambda_coef*(x-1)**2 + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bdcf6-9067-4d1e-b63a-66932f6aacfa",
   "metadata": {},
   "source": [
    "#### Comment 2\n",
    "In this way, setting the starting value as x0_reinforce\n",
    "sub_episode is the serial in a monte carlo model, so the mento carlo is running six time in each generation.\n",
    "in total \n",
    "sub_episode_length * sub_episode_num steps\n",
    "\n",
    "althoug the episode num need consider the parallel enviroment num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6af4c2-c54e-4ec4-8015-d691ce255221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 [-1.5 -1.5 -1.5]\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "#Set initial x-values for all three algorithms\n",
    "x0_reinforce = np.array([-1.5]*N,dtype=np.float32)\n",
    "final_reward   = -1e9\n",
    "final_solution = x0_reinforce.copy()    # so it always exists\n",
    "\n",
    "sub_episode_length = 30 #number of time_steps in a sub-episode. \n",
    "episode_length = sub_episode_length*10  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 6  #Number of parallel environments, each environment is used to generate an episode\n",
    "print('x0', x0_reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfe348d-436a-4c0c-98d3-ff84422f6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sub_episodes used for a single param update: 60\n"
     ]
    }
   ],
   "source": [
    "generation_num = 100  #number of theta updates for REINFORCE-IP\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e55af3-7385-45ba-98cf-9ed4df202743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.00001, C=100000)   \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7dc8f1-4480-4596-80c9-dfcb8f4c5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_min = np.array([-1]*N, dtype=np.int32)\n",
    "act_max = np.array([1]*N, dtype=np.int32)\n",
    "#x0_reinforce = act_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25b1acc-64b0-4670-abed-c1c216b3194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_num = 0\n",
    "step_size = np.array([0.1, 0.1, 0.1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33cd40c-bfec-416e-9253-e46e144710a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(x):\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f308304-5655-41ef-9e69-89cac8442630",
   "metadata": {},
   "source": [
    "#### Comment 3\n",
    "This part we have a key factor that distinguish to main project. use the MultiCategoricalProjectionNetwork\n",
    "to support the action here, in the origin project, the CategoricalProjectionNetwork only able to support the action dimensions\n",
    "that supports the same action space. This is not our case, we have tha action, require the different action space.\n",
    "There are three method to solve this,<br/>\n",
    "1, simply flat it to one array(flat_array.txt). https://github.com/tensorflow/agents/issues/702, but may account parameters explorion.<br/>\n",
    "2, use muti-head MultiCategoricalProjectionNetwork. https://gist.github.com/sidney-tio/66abada949f1b629dd9ee28777d402d5<br/>\n",
    "3, turing into the continous space<br/>\n",
    "\n",
    "The method might become parameters exploration if action space go so high, therefore here, we try the method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91475c0d-dc1c-408d-ad3e-40b5dba4261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multicategorical_projection import MultiCategoricalProjectionNetwork\n",
    "#from MultiCategoricalProjectionNetwork_T import MultiCategoricalProjectionNetwork_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fa91b-ef15-483d-a6b6-d6abbe722ab1",
   "metadata": {},
   "source": [
    "#### Comment 4\n",
    "It worth dive into the real update process in the step function below.<br/>\n",
    "The update is according to the equation: <br/>\n",
    "self._state = self._state + (action-act_max)*step_size <br/>\n",
    "then using the state calcualted the reward. <br/>\n",
    "R = compute_reward(self._state) <br/>\n",
    "The thought behind the behaviour is due to the directly use the action as state observation might create unstable duing to the spare samping, thus, instead replace the state with the change direction.\n",
    "after readjust the action space, act_max will work as the middle value to judge it is the postive or negative step size in next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fef87f-f4a8-4b3d-a9a9-fe54a442a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        #self._action_spec = array_spec.BoundedArraySpec(\n",
    "        #                    shape=(state_dim,), dtype=np.int32, \n",
    "        #                    minimum=np.array([0]*N), \n",
    "        #                    maximum=act_max-act_min, \n",
    "        #                    name='action') #a_t is an 2darray\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                                shape=(state_dim,),\n",
    "                                dtype=np.int32,\n",
    "                                minimum=np.array([0]*N),\n",
    "                                maximum=act_max-act_min,\n",
    "                                name='action')\n",
    "    \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "     \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        #Note that we set the action space as a set of non-negative vectors\n",
    "        #action-act_max converts the set to the desired set of negative vectors.\n",
    "\n",
    "        self._state = self._state + (action-act_max)*step_size    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fa1f1-3147-4747-947a-d3117e1296b6",
   "metadata": {},
   "source": [
    "##### for ENV test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c95f2-614a-45bf-8635-a98bdee15fe0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### test1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfb09c-a12b-444e-b609-1f2eced8403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f382a8a-733e-423e-9a4b-74e929379a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = test_env.reset()\n",
    "x0 = init_ts.observation.numpy()[0]  \n",
    "\n",
    "# construct all “unit” moves in each axis\n",
    "# e.g. if your discrete bins are [-3, -2, …, +3], Δ=±1 in each dim\n",
    "deltas = []\n",
    "for d in range(state_dim):\n",
    "    e = np.zeros_like(x0)\n",
    "    e[d] = 1   # +1 step in dim d\n",
    "    deltas.append(e)\n",
    "    deltas.append(-e)  # also try −1\n",
    "\n",
    "print(\"Flat-line test at x0 =\", x0)\n",
    "for Δ in deltas:\n",
    "    r0 = compute_reward(x0)\n",
    "    r1 = compute_reward(x0 + Δ)\n",
    "    print(f\"  Δ={Δ},  Δr = {r1 - r0:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745af216-1e69-49f5-82ff-0c988fbf421c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### test2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc64f9-89b7-428b-821b-458bddf9e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_action(spec):\n",
    "    \"\"\"Draw one uniform random integer action respecting a BoundedArraySpec.\"\"\"\n",
    "    # spec.minimum and spec.maximum may be scalars or arrays\n",
    "    low = np.array(spec.minimum, dtype=spec.dtype)\n",
    "    high = np.array(spec.maximum, dtype=spec.dtype)\n",
    "    # +1 because np.randint upper bound is exclusive\n",
    "    return np.random.randint(low, high + 1, size=spec.shape, dtype=spec.dtype)\n",
    "\n",
    "def test_env():\n",
    "    np.random.seed(0)\n",
    "    env = Env()\n",
    "\n",
    "    print(\"=== ACTION SPEC ===\")\n",
    "    print(env.action_spec())\n",
    "    print(\"\\n=== OBSERVATION SPEC ===\")\n",
    "    print(env.observation_spec())\n",
    "    print()\n",
    "\n",
    "    # 1) RESET \n",
    "    ts = env.reset()\n",
    "    print(\"RESET ->\")\n",
    "    print(\"  state    =\", ts.observation)\n",
    "    print(\"  reward   =\", ts.reward)\n",
    "    print(\"  stepType =\", ts.step_type)\n",
    "    print()\n",
    "\n",
    "    # 2) STEP RANDOMLY up to sub_episode_length+2\n",
    "    max_steps = 10\n",
    "    for i in range(max_steps):\n",
    "        a = sample_random_action(env.action_spec())\n",
    "        ts = env.step(a)\n",
    "        print(f\"STEP {i+1:2d}\")\n",
    "        print(\"  action   =\", a)\n",
    "        print(\"  new state=\", ts.observation)\n",
    "        print(\"  reward   =\", ts.reward)\n",
    "        print(\"  stepType =\", ts.step_type)\n",
    "        print()\n",
    "        if ts.step_type == StepType.LAST:\n",
    "            print(f\"Episode terminated at step {i+1}.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Did not terminate in {max_steps} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec40b54-fe99-471a-bb7a-df4502ecacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f664d7b-2827-4680-8c8a-c04e09e420b8",
   "metadata": {},
   "source": [
    "##### Env test END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff928bb-2989-4295-a268-ca83d26b90c0",
   "metadata": {},
   "source": [
    "#### Comment 5\n",
    "train and eval happen in the same iteration, eval is like a examination on the current train result, in this way got the extra debugging process to look at the chaos come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd49cf54-0bfd-4637-9d69-4e4f3d3b3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_env.batch_size = parallel environment number =  6\n"
     ]
    }
   ],
   "source": [
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=False) #instance\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)\n",
    "\n",
    "###Without the Parallel\n",
    "#train_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=True) #instance of parallel environments\n",
    "#eval_env = tf_py_environment.TFPyEnvironment(Env(), check_dims=False) #instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c531ed-7e45-4d7c-9df2-1d26bdd574f6",
   "metadata": {},
   "source": [
    "#### comment 6 How to set network\n",
    "On the how to set the neural network in setting.\n",
    "The neural network is create the following structure use the code for actor_net\n",
    "\n",
    "Input → Dense(10) → Activation(tanh) → Dense(5) → Activation(tanh) → [projection head]\n",
    "\n",
    "Input is the observation_spec, then the projection head is the action_spec. Unlike the supervised learning process, it trains with the forward and back prop with the loop. The reinforcement actually like a delay update, decouple the data collection(mento-calo process) and gradient update(using loss function to gradient update), the reward of vanilla reinforcement(pure Monte Carlo without bootstraping or n-step return these kind of policy optimal algorithm) only sum up in the end of the episode.\n",
    "\n",
    "in the discrete_projection_net, persumming we put a muti-dimesion predict layer here to predict the action, the place it put is after the Dense(5) layer above it send to this special neural layer.\n",
    "\n",
    "--Dense(5)-->   (batch,  5)  --tanh--> CategoricalNetwork -> projection head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c92b-7c31-4ce2-9619-b530ca687970",
   "metadata": {},
   "source": [
    "#### comment 7 CategoricalProjectionNetwork and MultiCategoricalProjectionNetwork\n",
    "The general idea of this Categorical network is to build the layer, that mapping to the all action point to the logits(the neural on the network to predicts the value), which suppose to be wired up in the policy network. The higher policy should able to do two seperate output, a log probability and a action sampler. \n",
    "\n",
    "The init() and call() function all invoke by the higher policy netwrok, like ActorDistributionNetwork here it's leave the set on the final layer it applied \"discrete_projection_net=\" before the projection head. actually the class CategoricalProjectionNetwork(network.DistributionNetwork), it use the inheritance here, then later it will reinvoke the paranet method by super(CategoricalProjectionNetwork, self).__init__. The actual init() should happened when intitial do the Network build ActorDistributionNetwork(...), although the call() function is invoked when the policy is on working scenario. The call() is warped up in the driver and the agent, in the <br/>\n",
    "action_step = collect_policy.action(time_step, policy_state)\n",
    "\n",
    "The main different is on the multi-head one, because the dimension are different, they need extra split on the netwrok. The detail can see in the _output_distribution_spec in the multi-head situation, it runs with the further MultiCategoricalDistributionBlock, use to spilt more sub-logits, according to the structure. Also there is the extra components in call function, it related to the mask(some special scenario to create the condtions), I believe this project do not have this feature here.\n",
    "\n",
    "extra: The observation and rewards, also with the different, consider the example in original reward, def f(x,lambda_coef=0.3):return -(x**2 -1)**2 - lambda_coef*(x-1)**2 + 5, actually able to return the as many dimension of the reward as the input action, in contrast, the scenario we want use the netwrok to reture a single reward value. like with np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00653e70-35ff-4777-952d-841d6b9e4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_distribution_network outputs a distribution\n",
    "#it is a neural net which outputs the parameter (mean and sd, named as loc and scale) for a normal distribution\n",
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(10,5), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         discrete_projection_net=MultiCategoricalProjectionNetwork,\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         #continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914728b8-8880-4695-a9bf-0ddf50eeb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "#        time_step_spec = train_env.time_step_spec(),\n",
    "#        action_spec = train_env.action_spec(),\n",
    "#        actor_network = actor_net,\n",
    "#        value_network = None,\n",
    "#        value_estimation_loss_coef = 0.2,\n",
    "#        optimizer = opt,\n",
    "#        advantage_fn = None,\n",
    "#        use_advantage_loss = False,\n",
    "#        gamma = 1.0, #discount factor for future returns\n",
    "#        normalize_returns = True, #The instruction says it's better to normalize\n",
    "#        gradient_clipping = 1.0,\n",
    "#        entropy_regularization = 0.05,\n",
    "#        train_step_counter = train_step_counter\n",
    "#        )\n",
    "\n",
    "\n",
    "#Original agent\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "       time_step_spec = train_env.time_step_spec(),\n",
    "       action_spec = train_env.action_spec(),\n",
    "       actor_network = actor_net,\n",
    "       value_network = None,\n",
    "       value_estimation_loss_coef = 0.2,\n",
    "       optimizer = opt,\n",
    "       advantage_fn = None,\n",
    "       use_advantage_loss = False,\n",
    "       gamma = 1.0, #discount factor for future returns\n",
    "       normalize_returns = False, #The instruction says it's better to normalize\n",
    "       gradient_clipping = None,\n",
    "       entropy_regularization = 0.01,\n",
    "       train_step_counter = train_step_counter\n",
    "       )\n",
    "\n",
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fded647-cc32-4588-a97b-e89a5e861ab3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Examine the actor net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fc05d-8bf0-47f2-9ab8-9776b1cb373d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# High-level Keras summary\n",
    "actor_net.summary()\n",
    "\n",
    "# Dig into the projection sub-network\n",
    "print(\"\\n=== projection sub-network ===\")\n",
    "proj_net = actor_net._projection_networks\n",
    "print(proj_net)\n",
    "# If it’s a Keras Model, you can do:\n",
    "try:\n",
    "    proj_net.summary()\n",
    "except Exception:\n",
    "    # not all TF-Agents networks expose .summary()\n",
    "    print(\"No .summary() on this object; listing its layers instead:\")\n",
    "    for layer in proj_net.layers:\n",
    "        print(\" \", layer.name, layer)\n",
    "\n",
    "# Finally, list all trainable variables so you see exactly which weight matrices live where\n",
    "print(\"\\n=== trainable variables ===\")\n",
    "for v in actor_net.trainable_variables:\n",
    "    print(v.name, v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3866a-731d-41be-a462-1798b3f765ff",
   "metadata": {},
   "source": [
    "##### end of examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581cba1-fbe5-4c65-9ca6-a292047efbe1",
   "metadata": {},
   "source": [
    "#### Comment 7 The max_length has optimised\n",
    "Left the episode_length*env_num*2 rather than episode_length*100 should large enough for the headroom.\n",
    "reply buffer use in the trainning procedure, and clean it self in each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc354fb-5e6c-4921-b16e-70cf26016bce",
   "metadata": {},
   "source": [
    "##### Agent Policy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f22ff-ce6f-4959-894b-5619634dee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2) Sampling vs. greedy policy check\n",
    "##Verify you really are using a stochastic policy for collection, and that it actually samples other bins early on:\n",
    "##If all 10 samples equal greedy (or all equal your initial bin), then you’re accidentally collecting with the deterministic policy or your logits already collapsed.\n",
    "\n",
    "ts = train_env.reset()\n",
    "print(\"Greedy vs. sampled at initial state:\")\n",
    "# greedy “best‐guess” action\n",
    "greedy = REINFORCE_agent.policy.action(ts).action.numpy()\n",
    "print(\"  greedy action =\", greedy)\n",
    "\n",
    "# 10 stochastic samples from collect_policy\n",
    "for i in range(10):\n",
    "    a = REINFORCE_agent.collect_policy.action(ts).action.numpy()\n",
    "    print(f\"  sample {i:2d} →\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffceab0f-c839-4aeb-b3c5-70fc05b21708",
   "metadata": {},
   "source": [
    "##### End Policy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45605d6-b5c2-4b92-b9ae-ed5575377489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                \n",
    "                max_length = 100    \n",
    "    \n",
    "                # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "#test_buffer is used for evaluating a policy\n",
    "test_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec= REINFORCE_agent.collect_data_spec,  # describe a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size= 1,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                                    # train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "                                                    # batch_size: Batch dimension of tensors when adding to buffer. \n",
    "                max_length = episode_length         # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c050cc-7da9-4fd3-b0d3-35cdce6b36f0",
   "metadata": {},
   "source": [
    "#### Comment 8\n",
    "In which way to select a proper policy. The chosen on the driver is not the main concern. It is mainly how you want to trainning in the which steps level. DynamicEpisodeDriver is more suitabale for trainning in the parallel env, pydriver is sutiable to the single episode, there are other drivers existing such as,  DynamicStepDriver(able to control the exactly how many steps want to run), onstepdriver(only run one steps). \n",
    "\n",
    "really distinguish the trainning and evaluation is on how to set the policy dependence.<br/>\n",
    "policy = REINFORCE_agent.collect_policy, stochastic policy<br/>\n",
    "policy = REINFORCE_agent.policy,greedy policy<br/>\n",
    "\n",
    "generally, the trainning is on the stochastic policy collection, but the evaluation is on the greedy policy picks arg⁡max⁡𝑎 𝜋𝜃(𝑎∣𝑠)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75222c56-ee74-4935-9fba-e07cbbb2fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                     env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                     policy = REINFORCE_agent.collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )\n",
    "\n",
    "#For policy evaluation\n",
    "test_driver = py_driver.PyDriver(\n",
    "                                     env = eval_env, #PyEnvironment or TFEnvironment class\n",
    "                                     policy = REINFORCE_agent.policy,\n",
    "                                     observers = [test_buffer.add_batch],\n",
    "                                     max_episodes=1, #optional. If provided, the data generation ends whenever\n",
    "                                                      #either max_steps or max_episodes is reached.\n",
    "                                     max_steps=sub_episode_length\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbd7295-a888-459c-aec7-dc1a70361ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e75fb-5fb8-4e5d-9c4c-420f4dbf4f84",
   "metadata": {},
   "source": [
    "##### Test Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547ccad-6318-49db-99ef-0341e859d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = 3\n",
    "eval_intv = 1 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "\n",
    "\n",
    "        ########################Check the action distribution\n",
    "        #######\n",
    "        logits = actions_distribution.parameters[\"logits\"]\n",
    "        \n",
    "        print(\"min / max / mean logits =\",\n",
    "              tf.reduce_min(logits).numpy(),\n",
    "              tf.reduce_max(logits).numpy(),\n",
    "              tf.reduce_mean(logits).numpy())\n",
    "        #######\n",
    "        ########################\n",
    "        \n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "\n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        #print('collect_traj:', observations[0,:,0].numpy())\n",
    "        ##  I persume this means the a random trajectories give in the scenario\n",
    "        print('collect_traj:', observations[0,:].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcabb5c-3765-4f79-8e39-f82b0d16d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4a3d4-ec90-4675-a9b6-7aa45a812fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations = observations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc8d23-f76b-4324-9efc-cb7bb7324a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7403109-51a5-4189-9aa6-97ad71abe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90587243-b892-4558-a702-22eff75bea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_observations[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bde81-dc24-4be5-998d-b2b26b854526",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(np.array([-2.6,-2.3,-2.7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c1324-68db-4425-9055-fd27c33c0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 1, 1]-act_max)*step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fbe62-4d5f-469e-aaf9-42446981683e",
   "metadata": {},
   "source": [
    "##### end of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ed32e-eb82-4862-9572-c2acfee81263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72d5655-3b22-4ed2-9df5-bbd9088389f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step   0] mean entropy per-dim = 3.1601\n",
      "train_step no.= 1001\n",
      "best_solution of this generation= [-0.8 -0.8 -0.9]\n",
      "best step reward= 11.678 11.6777\n",
      "avg step reward= 8.91\n",
      "best_step_index: [16, 6]\n",
      "collect_traj: [[-1.2        -1.1        -0.59999996]\n",
      " [-1.1        -1.2        -0.7       ]\n",
      " [-1.1        -1.1        -0.59999996]\n",
      " [-1.         -1.1        -0.5       ]\n",
      " [-1.         -1.         -0.39999998]\n",
      " [-0.9        -1.         -0.5       ]\n",
      " [-1.         -0.9        -0.39999998]\n",
      " [-0.9        -0.9        -0.29999998]\n",
      " [-1.         -0.9        -0.19999999]\n",
      " [-1.         -0.9        -0.29999998]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.6        -1.4        -1.4       ]\n",
      " [-1.5        -1.3        -1.4       ]\n",
      " [-1.4        -1.2        -1.4       ]\n",
      " [-1.3        -1.3        -1.4       ]\n",
      " [-1.2        -1.4        -1.3       ]\n",
      " [-1.1        -1.4        -1.2       ]\n",
      " [-1.         -1.3        -1.3       ]\n",
      " [-1.         -1.2        -1.3       ]\n",
      " [-0.9        -1.2        -1.2       ]\n",
      " [-0.8        -1.3        -1.2       ]\n",
      " [-0.7        -1.2        -1.2       ]\n",
      " [-0.8        -1.2        -1.1       ]\n",
      " [-0.7        -1.1        -1.        ]\n",
      " [-0.59999996 -1.         -1.1       ]\n",
      " [-0.5        -1.         -1.        ]\n",
      " [-0.5        -0.9        -1.1       ]\n",
      " [-0.5        -1.         -1.        ]\n",
      " [-0.5        -1.1        -0.9       ]\n",
      " [-0.39999998 -1.2        -0.9       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  5.00000000e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  4.00000036e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.40000010e+00]]\n",
      "[step   1] mean entropy per-dim = 3.1616\n",
      "[step   2] mean entropy per-dim = 3.1564\n",
      "[step   3] mean entropy per-dim = 3.1566\n",
      "[step   4] mean entropy per-dim = 3.1617\n",
      "[step   5] mean entropy per-dim = 3.1605\n",
      "[step   6] mean entropy per-dim = 3.1559\n",
      "[step   7] mean entropy per-dim = 3.1559\n",
      "[step   8] mean entropy per-dim = 3.1590\n",
      "[step   9] mean entropy per-dim = 3.1577\n",
      "[step  10] mean entropy per-dim = 3.1591\n",
      "[step  11] mean entropy per-dim = 3.1572\n",
      "[step  12] mean entropy per-dim = 3.1570\n",
      "[step  13] mean entropy per-dim = 3.1581\n",
      "[step  14] mean entropy per-dim = 3.1601\n",
      "[step  15] mean entropy per-dim = 3.1608\n",
      "[step  16] mean entropy per-dim = 3.1571\n",
      "[step  17] mean entropy per-dim = 3.1553\n",
      "[step  18] mean entropy per-dim = 3.1596\n",
      "[step  19] mean entropy per-dim = 3.1580\n",
      "[step  20] mean entropy per-dim = 3.1560\n",
      "[step  21] mean entropy per-dim = 3.1611\n",
      "[step  22] mean entropy per-dim = 3.1611\n",
      "[step  23] mean entropy per-dim = 3.1539\n",
      "[step  24] mean entropy per-dim = 3.1563\n",
      "[step  25] mean entropy per-dim = 3.1590\n",
      "[step  26] mean entropy per-dim = 3.1545\n",
      "[step  27] mean entropy per-dim = 3.1596\n",
      "[step  28] mean entropy per-dim = 3.1579\n",
      "[step  29] mean entropy per-dim = 3.1547\n",
      "[step  30] mean entropy per-dim = 3.1557\n",
      "train_step no.= 1031\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 8.489\n",
      "best_step_index: [15, 29]\n",
      "collect_traj: [[-1.2        -0.8        -1.5       ]\n",
      " [-1.1        -0.9        -1.4       ]\n",
      " [-1.1        -0.8        -1.5       ]\n",
      " [-1.         -0.7        -1.4       ]\n",
      " [-1.         -0.8        -1.3       ]\n",
      " [-1.         -0.9        -1.2       ]\n",
      " [-1.         -0.9        -1.2       ]\n",
      " [-1.         -0.8        -1.1       ]\n",
      " [-1.1        -0.7        -1.        ]\n",
      " [-1.1        -0.7        -1.1       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.5        -1.4       ]\n",
      " [-1.5        -1.4        -1.3       ]\n",
      " [-1.6        -1.3        -1.3       ]\n",
      " [-1.7        -1.4        -1.3       ]\n",
      " [-1.7        -1.5        -1.2       ]\n",
      " [-1.6        -1.5        -1.1       ]\n",
      " [-1.6        -1.4        -1.1       ]\n",
      " [-1.5        -1.3        -1.        ]\n",
      " [-1.4        -1.2        -0.9       ]\n",
      " [-1.5        -1.3        -0.8       ]\n",
      " [-1.4        -1.2        -0.9       ]\n",
      " [-1.3        -1.2        -0.8       ]\n",
      " [-1.4        -1.3        -0.7       ]\n",
      " [-1.3        -1.2        -0.8       ]\n",
      " [-1.3        -1.3        -0.9       ]\n",
      " [-1.3        -1.4        -0.8       ]\n",
      " [-1.2        -1.5        -0.7       ]\n",
      " [-1.2        -1.4        -0.7       ]\n",
      " [-1.2        -1.3        -0.59999996]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  5.00000000e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  4.00000036e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.40000010e+00]]\n",
      "[step  31] mean entropy per-dim = 3.1517\n",
      "[step  32] mean entropy per-dim = 3.1626\n",
      "[step  33] mean entropy per-dim = 3.1558\n",
      "[step  34] mean entropy per-dim = 3.1561\n",
      "[step  35] mean entropy per-dim = 3.1549\n",
      "[step  36] mean entropy per-dim = 3.1543\n",
      "[step  37] mean entropy per-dim = 3.1581\n",
      "[step  38] mean entropy per-dim = 3.1555\n",
      "[step  39] mean entropy per-dim = 3.1485\n",
      "[step  40] mean entropy per-dim = 3.1528\n",
      "[step  41] mean entropy per-dim = 3.1564\n",
      "[step  42] mean entropy per-dim = 3.1494\n",
      "[step  43] mean entropy per-dim = 3.1552\n",
      "[step  44] mean entropy per-dim = 3.1484\n",
      "[step  45] mean entropy per-dim = 3.1587\n",
      "[step  46] mean entropy per-dim = 3.1533\n",
      "[step  47] mean entropy per-dim = 3.1551\n",
      "[step  48] mean entropy per-dim = 3.1506\n",
      "[step  49] mean entropy per-dim = 3.1552\n",
      "[step  50] mean entropy per-dim = 3.1553\n",
      "[step  51] mean entropy per-dim = 3.1581\n",
      "[step  52] mean entropy per-dim = 3.1513\n",
      "[step  53] mean entropy per-dim = 3.1531\n",
      "[step  54] mean entropy per-dim = 3.1564\n",
      "[step  55] mean entropy per-dim = 3.1541\n",
      "[step  56] mean entropy per-dim = 3.1557\n",
      "[step  57] mean entropy per-dim = 3.1592\n",
      "[step  58] mean entropy per-dim = 3.1530\n",
      "[step  59] mean entropy per-dim = 3.1534\n",
      "[step  60] mean entropy per-dim = 3.1528\n",
      "train_step no.= 1061\n",
      "best_solution of this generation= [-0.9 -0.8 -0.8]\n",
      "best step reward= 11.678 11.6777\n",
      "avg step reward= 8.55\n",
      "best_step_index: [2, 2]\n",
      "collect_traj: [[-1.1        -1.         -0.8       ]\n",
      " [-1.         -1.         -0.8       ]\n",
      " [-0.9        -1.         -0.9       ]\n",
      " [-0.9        -1.1        -1.        ]\n",
      " [-0.8        -1.         -1.        ]\n",
      " [-0.8        -0.9        -0.9       ]\n",
      " [-0.8        -1.         -0.9       ]\n",
      " [-0.7        -0.9        -1.        ]\n",
      " [-0.59999996 -0.8        -1.        ]\n",
      " [-0.59999996 -0.7        -1.1       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.6        -1.6        -1.4       ]\n",
      " [-1.6        -1.6        -1.5       ]\n",
      " [-1.5        -1.6        -1.6       ]\n",
      " [-1.4        -1.5        -1.6       ]\n",
      " [-1.3        -1.4        -1.5       ]\n",
      " [-1.3        -1.4        -1.5       ]\n",
      " [-1.3        -1.5        -1.5       ]\n",
      " [-1.3        -1.5        -1.4       ]\n",
      " [-1.2        -1.4        -1.4       ]\n",
      " [-1.1        -1.4        -1.5       ]\n",
      " [-1.2        -1.5        -1.4       ]\n",
      " [-1.3        -1.4        -1.5       ]\n",
      " [-1.2        -1.3        -1.4       ]\n",
      " [-1.2        -1.3        -1.3       ]\n",
      " [-1.1        -1.2        -1.3       ]\n",
      " [-1.2        -1.1        -1.2       ]\n",
      " [-1.2        -1.1        -1.1       ]\n",
      " [-1.1        -1.2        -1.1       ]\n",
      " [-1.1        -1.3        -1.        ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  5.00000000e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  4.00000036e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.40000010e+00]]\n",
      "[step  61] mean entropy per-dim = 3.1520\n",
      "[step  62] mean entropy per-dim = 3.1508\n",
      "[step  63] mean entropy per-dim = 3.1563\n",
      "final reward before udpate: 12.0937\n",
      "final reward after udpate: 12.2962\n",
      "updated final_solution= [-0.9 -0.7  0.6]\n",
      "[step  64] mean entropy per-dim = 3.1508\n",
      "[step  65] mean entropy per-dim = 3.1538\n",
      "[step  66] mean entropy per-dim = 3.1468\n",
      "[step  67] mean entropy per-dim = 3.1534\n",
      "[step  68] mean entropy per-dim = 3.1542\n",
      "[step  69] mean entropy per-dim = 3.1538\n",
      "[step  70] mean entropy per-dim = 3.1546\n",
      "[step  71] mean entropy per-dim = 3.1529\n",
      "[step  72] mean entropy per-dim = 3.1499\n",
      "[step  73] mean entropy per-dim = 3.1513\n",
      "[step  74] mean entropy per-dim = 3.1528\n",
      "[step  75] mean entropy per-dim = 3.1493\n",
      "[step  76] mean entropy per-dim = 3.1512\n",
      "[step  77] mean entropy per-dim = 3.1471\n",
      "[step  78] mean entropy per-dim = 3.1569\n",
      "[step  79] mean entropy per-dim = 3.1494\n",
      "[step  80] mean entropy per-dim = 3.1501\n",
      "[step  81] mean entropy per-dim = 3.1463\n",
      "[step  82] mean entropy per-dim = 3.1482\n",
      "[step  83] mean entropy per-dim = 3.1528\n",
      "[step  84] mean entropy per-dim = 3.1440\n",
      "[step  85] mean entropy per-dim = 3.1514\n",
      "[step  86] mean entropy per-dim = 3.1487\n",
      "[step  87] mean entropy per-dim = 3.1453\n",
      "[step  88] mean entropy per-dim = 3.1497\n",
      "[step  89] mean entropy per-dim = 3.1507\n",
      "[step  90] mean entropy per-dim = 3.1481\n",
      "train_step no.= 1091\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 8.296\n",
      "best_step_index: [5, 23]\n",
      "collect_traj: [[-1.1        -1.3        -1.        ]\n",
      " [-1.2        -1.3        -1.        ]\n",
      " [-1.1        -1.2        -0.9       ]\n",
      " [-1.2        -1.3        -0.9       ]\n",
      " [-1.3        -1.3        -1.        ]\n",
      " [-1.3        -1.2        -1.        ]\n",
      " [-1.4        -1.3        -0.9       ]\n",
      " [-1.3        -1.2        -0.9       ]\n",
      " [-1.2        -1.1        -0.8       ]\n",
      " [-1.3        -1.2        -0.7       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.5        -1.5       ]\n",
      " [-1.3        -1.5        -1.5       ]\n",
      " [-1.3        -1.5        -1.4       ]\n",
      " [-1.2        -1.5        -1.5       ]\n",
      " [-1.1        -1.4        -1.5       ]\n",
      " [-1.         -1.4        -1.5       ]\n",
      " [-1.         -1.3        -1.4       ]\n",
      " [-1.         -1.2        -1.3       ]\n",
      " [-1.         -1.2        -1.2       ]\n",
      " [-0.9        -1.1        -1.3       ]\n",
      " [-0.9        -1.         -1.2       ]\n",
      " [-1.         -1.         -1.1       ]\n",
      " [-0.9        -1.1        -1.        ]\n",
      " [-1.         -1.         -0.9       ]\n",
      " [-1.         -1.         -0.8       ]\n",
      " [-0.9        -0.9        -0.7       ]\n",
      " [-0.9        -0.9        -0.59999996]\n",
      " [-0.8        -0.8        -0.59999996]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  5.00000000e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  4.00000036e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.40000010e+00]]\n",
      "[step  91] mean entropy per-dim = 3.1497\n",
      "[step  92] mean entropy per-dim = 3.1505\n",
      "[step  93] mean entropy per-dim = 3.1520\n",
      "[step  94] mean entropy per-dim = 3.1513\n",
      "[step  95] mean entropy per-dim = 3.1470\n",
      "[step  96] mean entropy per-dim = 3.1454\n",
      "[step  97] mean entropy per-dim = 3.1431\n",
      "[step  98] mean entropy per-dim = 3.1433\n",
      "[step  99] mean entropy per-dim = 3.1484\n",
      "[step 100] mean entropy per-dim = 3.1448\n",
      "[step 101] mean entropy per-dim = 3.1429\n",
      "[step 102] mean entropy per-dim = 3.1480\n",
      "[step 103] mean entropy per-dim = 3.1488\n",
      "[step 104] mean entropy per-dim = 3.1468\n",
      "[step 105] mean entropy per-dim = 3.1478\n",
      "[step 106] mean entropy per-dim = 3.1466\n",
      "[step 107] mean entropy per-dim = 3.1426\n",
      "[step 108] mean entropy per-dim = 3.1407\n",
      "[step 109] mean entropy per-dim = 3.1386\n",
      "[step 110] mean entropy per-dim = 3.1421\n",
      "[step 111] mean entropy per-dim = 3.1387\n",
      "[step 112] mean entropy per-dim = 3.1458\n",
      "[step 113] mean entropy per-dim = 3.1447\n",
      "[step 114] mean entropy per-dim = 3.1401\n",
      "[step 115] mean entropy per-dim = 3.1398\n",
      "[step 116] mean entropy per-dim = 3.1354\n",
      "[step 117] mean entropy per-dim = 3.1419\n",
      "[step 118] mean entropy per-dim = 3.1425\n",
      "[step 119] mean entropy per-dim = 3.1385\n",
      "[step 120] mean entropy per-dim = 3.1436\n",
      "train_step no.= 1121\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 9.07\n",
      "best_step_index: [6, 1]\n",
      "collect_traj: [[-0.7        -0.9        -1.3       ]\n",
      " [-0.59999996 -0.8        -1.2       ]\n",
      " [-0.7        -0.7        -1.1       ]\n",
      " [-0.8        -0.7        -1.2       ]\n",
      " [-0.8        -0.8        -1.1       ]\n",
      " [-0.8        -0.8        -1.        ]\n",
      " [-0.7        -0.7        -0.9       ]\n",
      " [-0.59999996 -0.8        -0.8       ]\n",
      " [-0.5        -0.7        -0.8       ]\n",
      " [-0.5        -0.7        -0.8       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.6        -1.4       ]\n",
      " [-1.3        -1.7        -1.4       ]\n",
      " [-1.2        -1.6        -1.3       ]\n",
      " [-1.2        -1.7        -1.3       ]\n",
      " [-1.1        -1.6        -1.2       ]\n",
      " [-1.         -1.6        -1.2       ]\n",
      " [-1.1        -1.5        -1.2       ]\n",
      " [-1.         -1.4        -1.1       ]\n",
      " [-0.9        -1.3        -1.        ]\n",
      " [-1.         -1.3        -1.        ]\n",
      " [-1.1        -1.3        -0.9       ]\n",
      " [-1.2        -1.3        -0.8       ]\n",
      " [-1.2        -1.2        -0.7       ]\n",
      " [-1.1        -1.1        -0.59999996]\n",
      " [-1.2        -1.         -0.5       ]\n",
      " [-1.2        -1.         -0.59999996]\n",
      " [-1.2        -1.         -0.59999996]\n",
      " [-1.1        -1.1        -0.7       ]\n",
      " [-1.2        -1.         -0.59999996]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  5.00000000e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  4.00000036e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.40000010e+00]]\n",
      "[step 121] mean entropy per-dim = 3.1394\n",
      "[step 122] mean entropy per-dim = 3.1428\n",
      "[step 123] mean entropy per-dim = 3.1403\n",
      "[step 124] mean entropy per-dim = 3.1410\n",
      "[step 125] mean entropy per-dim = 3.1408\n",
      "[step 126] mean entropy per-dim = 3.1362\n",
      "[step 127] mean entropy per-dim = 3.1359\n",
      "[step 128] mean entropy per-dim = 3.1388\n",
      "[step 129] mean entropy per-dim = 3.1338\n",
      "[step 130] mean entropy per-dim = 3.1382\n",
      "[step 131] mean entropy per-dim = 3.1391\n",
      "[step 132] mean entropy per-dim = 3.1351\n",
      "[step 133] mean entropy per-dim = 3.1371\n",
      "[step 134] mean entropy per-dim = 3.1337\n",
      "[step 135] mean entropy per-dim = 3.1316\n",
      "[step 136] mean entropy per-dim = 3.1386\n",
      "[step 137] mean entropy per-dim = 3.1397\n",
      "[step 138] mean entropy per-dim = 3.1342\n",
      "[step 139] mean entropy per-dim = 3.1339\n",
      "[step 140] mean entropy per-dim = 3.1333\n",
      "[step 141] mean entropy per-dim = 3.1380\n",
      "[step 142] mean entropy per-dim = 3.1319\n",
      "[step 143] mean entropy per-dim = 3.1338\n",
      "[step 144] mean entropy per-dim = 3.1338\n",
      "[step 145] mean entropy per-dim = 3.1335\n",
      "[step 146] mean entropy per-dim = 3.1324\n",
      "[step 147] mean entropy per-dim = 3.1366\n",
      "[step 148] mean entropy per-dim = 3.1338\n",
      "[step 149] mean entropy per-dim = 3.1331\n",
      "[step 150] mean entropy per-dim = 3.1297\n",
      "train_step no.= 1151\n",
      "best_solution of this generation= [-0.9 -1.   0.3]\n",
      "best step reward= 11.706 11.705799\n",
      "avg step reward= 8.658\n",
      "best_step_index: [6, 9]\n",
      "collect_traj: [[-1.2        -1.         -1.        ]\n",
      " [-1.1        -0.9        -0.9       ]\n",
      " [-1.         -0.9        -0.8       ]\n",
      " [-1.         -0.9        -0.7       ]\n",
      " [-1.         -1.         -0.59999996]\n",
      " [-0.9        -1.         -0.59999996]\n",
      " [-0.8        -1.         -0.59999996]\n",
      " [-0.7        -0.9        -0.59999996]\n",
      " [-0.7        -0.9        -0.5       ]\n",
      " [-0.7        -0.9        -0.5       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.4        -1.5       ]\n",
      " [-1.3        -1.4        -1.6       ]\n",
      " [-1.2        -1.5        -1.6       ]\n",
      " [-1.2        -1.5        -1.5       ]\n",
      " [-1.1        -1.6        -1.5       ]\n",
      " [-1.2        -1.5        -1.5       ]\n",
      " [-1.1        -1.6        -1.4       ]\n",
      " [-1.         -1.5        -1.5       ]\n",
      " [-0.9        -1.4        -1.4       ]\n",
      " [-0.8        -1.4        -1.4       ]\n",
      " [-0.7        -1.4        -1.4       ]\n",
      " [-0.8        -1.4        -1.5       ]\n",
      " [-0.9        -1.5        -1.4       ]\n",
      " [-0.8        -1.4        -1.3       ]\n",
      " [-0.8        -1.4        -1.2       ]\n",
      " [-0.7        -1.4        -1.1       ]\n",
      " [-0.8        -1.3        -1.        ]\n",
      " [-0.7        -1.2        -1.1       ]\n",
      " [-0.59999996 -1.3        -1.        ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  7.00000048e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  8.00000012e-01]\n",
      " [-2.99999982e-01  5.00000000e-01  9.00000036e-01]\n",
      " [-3.99999976e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.20000005e+00]]\n",
      "[step 151] mean entropy per-dim = 3.1329\n",
      "[step 152] mean entropy per-dim = 3.1313\n",
      "[step 153] mean entropy per-dim = 3.1372\n",
      "[step 154] mean entropy per-dim = 3.1365\n",
      "[step 155] mean entropy per-dim = 3.1296\n",
      "[step 156] mean entropy per-dim = 3.1316\n",
      "[step 157] mean entropy per-dim = 3.1300\n",
      "[step 158] mean entropy per-dim = 3.1345\n",
      "[step 159] mean entropy per-dim = 3.1281\n",
      "[step 160] mean entropy per-dim = 3.1290\n",
      "[step 161] mean entropy per-dim = 3.1379\n",
      "[step 162] mean entropy per-dim = 3.1311\n",
      "[step 163] mean entropy per-dim = 3.1308\n",
      "[step 164] mean entropy per-dim = 3.1295\n",
      "[step 165] mean entropy per-dim = 3.1278\n",
      "[step 166] mean entropy per-dim = 3.1330\n",
      "[step 167] mean entropy per-dim = 3.1290\n",
      "[step 168] mean entropy per-dim = 3.1305\n",
      "[step 169] mean entropy per-dim = 3.1263\n",
      "[step 170] mean entropy per-dim = 3.1263\n",
      "[step 171] mean entropy per-dim = 3.1280\n",
      "[step 172] mean entropy per-dim = 3.1292\n",
      "[step 173] mean entropy per-dim = 3.1232\n",
      "[step 174] mean entropy per-dim = 3.1267\n",
      "[step 175] mean entropy per-dim = 3.1243\n",
      "[step 176] mean entropy per-dim = 3.1305\n",
      "[step 177] mean entropy per-dim = 3.1217\n",
      "[step 178] mean entropy per-dim = 3.1280\n",
      "[step 179] mean entropy per-dim = 3.1245\n",
      "[step 180] mean entropy per-dim = 3.1232\n",
      "train_step no.= 1181\n",
      "best_solution of this generation= [-0.8 -0.8 -0.9]\n",
      "best step reward= 11.678 11.6777\n",
      "avg step reward= 8.634\n",
      "best_step_index: [12, 1]\n",
      "collect_traj: [[-1.1000000e+00 -5.0000000e-01 -8.9999998e-01]\n",
      " [-1.1000000e+00 -3.9999998e-01 -8.0000001e-01]\n",
      " [-1.0000000e+00 -2.9999998e-01 -8.0000001e-01]\n",
      " [-8.9999998e-01 -1.9999999e-01 -6.9999999e-01]\n",
      " [-1.0000000e+00 -9.9999979e-02 -5.9999996e-01]\n",
      " [-8.9999998e-01 -9.9999979e-02 -5.0000000e-01]\n",
      " [-1.0000000e+00  2.2351742e-08 -5.0000000e-01]\n",
      " [-1.1000000e+00 -9.9999979e-02 -3.9999998e-01]\n",
      " [-1.0000000e+00 -1.9999999e-01 -2.9999998e-01]\n",
      " [-8.9999998e-01 -9.9999979e-02 -1.9999999e-01]\n",
      " [-1.5000000e+00 -1.5000000e+00 -1.5000000e+00]\n",
      " [-1.4000000e+00 -1.4000000e+00 -1.5000000e+00]\n",
      " [-1.3000000e+00 -1.4000000e+00 -1.6000000e+00]\n",
      " [-1.3000000e+00 -1.3000000e+00 -1.6000000e+00]\n",
      " [-1.3000000e+00 -1.3000000e+00 -1.6000000e+00]\n",
      " [-1.2000000e+00 -1.3000000e+00 -1.5000000e+00]\n",
      " [-1.2000000e+00 -1.3000000e+00 -1.6000000e+00]\n",
      " [-1.2000000e+00 -1.4000000e+00 -1.7000000e+00]\n",
      " [-1.3000000e+00 -1.3000000e+00 -1.6000000e+00]\n",
      " [-1.3000000e+00 -1.4000000e+00 -1.6000000e+00]\n",
      " [-1.4000000e+00 -1.3000000e+00 -1.5000000e+00]\n",
      " [-1.5000000e+00 -1.2000000e+00 -1.4000000e+00]\n",
      " [-1.4000000e+00 -1.2000000e+00 -1.3000000e+00]\n",
      " [-1.4000000e+00 -1.2000000e+00 -1.3000000e+00]\n",
      " [-1.4000000e+00 -1.1000000e+00 -1.2000000e+00]\n",
      " [-1.5000000e+00 -1.1000000e+00 -1.3000000e+00]\n",
      " [-1.4000000e+00 -1.2000000e+00 -1.4000000e+00]\n",
      " [-1.4000000e+00 -1.2000000e+00 -1.3000000e+00]\n",
      " [-1.3000000e+00 -1.1000000e+00 -1.4000000e+00]\n",
      " [-1.4000000e+00 -1.0000000e+00 -1.5000000e+00]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  8.00000012e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  7.00000048e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  8.00000012e-01]\n",
      " [-2.99999982e-01  5.00000000e-01  9.00000036e-01]\n",
      " [-3.99999976e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.20000005e+00]]\n",
      "[step 181] mean entropy per-dim = 3.1197\n",
      "[step 182] mean entropy per-dim = 3.1224\n",
      "[step 183] mean entropy per-dim = 3.1252\n",
      "[step 184] mean entropy per-dim = 3.1216\n",
      "[step 185] mean entropy per-dim = 3.1251\n",
      "[step 186] mean entropy per-dim = 3.1246\n",
      "[step 187] mean entropy per-dim = 3.1281\n",
      "[step 188] mean entropy per-dim = 3.1239\n",
      "[step 189] mean entropy per-dim = 3.1283\n",
      "[step 190] mean entropy per-dim = 3.1222\n",
      "[step 191] mean entropy per-dim = 3.1204\n",
      "[step 192] mean entropy per-dim = 3.1185\n",
      "[step 193] mean entropy per-dim = 3.1171\n",
      "[step 194] mean entropy per-dim = 3.1300\n",
      "[step 195] mean entropy per-dim = 3.1258\n",
      "[step 196] mean entropy per-dim = 3.1273\n",
      "[step 197] mean entropy per-dim = 3.1239\n",
      "[step 198] mean entropy per-dim = 3.1204\n",
      "[step 199] mean entropy per-dim = 3.1252\n",
      "[step 200] mean entropy per-dim = 3.1181\n",
      "[step 201] mean entropy per-dim = 3.1265\n",
      "[step 202] mean entropy per-dim = 3.1244\n",
      "[step 203] mean entropy per-dim = 3.1211\n",
      "[step 204] mean entropy per-dim = 3.1229\n",
      "[step 205] mean entropy per-dim = 3.1207\n",
      "[step 206] mean entropy per-dim = 3.1182\n",
      "[step 207] mean entropy per-dim = 3.1205\n",
      "[step 208] mean entropy per-dim = 3.1189\n",
      "[step 209] mean entropy per-dim = 3.1169\n",
      "[step 210] mean entropy per-dim = 3.1182\n",
      "train_step no.= 1211\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 8.615\n",
      "best_step_index: [3, 27]\n",
      "collect_traj: [[-1.3        -0.9        -1.        ]\n",
      " [-1.2        -0.8        -0.9       ]\n",
      " [-1.1        -0.9        -0.9       ]\n",
      " [-1.         -0.8        -0.8       ]\n",
      " [-1.         -0.7        -0.7       ]\n",
      " [-0.9        -0.59999996 -0.7       ]\n",
      " [-1.         -0.59999996 -0.59999996]\n",
      " [-0.9        -0.59999996 -0.59999996]\n",
      " [-0.8        -0.59999996 -0.5       ]\n",
      " [-0.8        -0.59999996 -0.5       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.5        -1.4       ]\n",
      " [-1.4        -1.4        -1.3       ]\n",
      " [-1.5        -1.5        -1.3       ]\n",
      " [-1.5        -1.4        -1.2       ]\n",
      " [-1.6        -1.5        -1.2       ]\n",
      " [-1.6        -1.6        -1.3       ]\n",
      " [-1.5        -1.5        -1.2       ]\n",
      " [-1.4        -1.6        -1.3       ]\n",
      " [-1.5        -1.6        -1.3       ]\n",
      " [-1.5        -1.5        -1.3       ]\n",
      " [-1.5        -1.5        -1.2       ]\n",
      " [-1.4        -1.4        -1.1       ]\n",
      " [-1.4        -1.4        -1.        ]\n",
      " [-1.3        -1.3        -1.1       ]\n",
      " [-1.2        -1.4        -1.        ]\n",
      " [-1.1        -1.4        -1.        ]\n",
      " [-1.         -1.3        -0.9       ]\n",
      " [-0.9        -1.3        -0.9       ]\n",
      " [-0.8        -1.3        -0.8       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 211] mean entropy per-dim = 3.1226\n",
      "[step 212] mean entropy per-dim = 3.1255\n",
      "[step 213] mean entropy per-dim = 3.1196\n",
      "[step 214] mean entropy per-dim = 3.1207\n",
      "[step 215] mean entropy per-dim = 3.1208\n",
      "[step 216] mean entropy per-dim = 3.1213\n",
      "[step 217] mean entropy per-dim = 3.1204\n",
      "[step 218] mean entropy per-dim = 3.1174\n",
      "[step 219] mean entropy per-dim = 3.1189\n",
      "[step 220] mean entropy per-dim = 3.1175\n",
      "[step 221] mean entropy per-dim = 3.1206\n",
      "[step 222] mean entropy per-dim = 3.1170\n",
      "[step 223] mean entropy per-dim = 3.1185\n",
      "[step 224] mean entropy per-dim = 3.1163\n",
      "[step 225] mean entropy per-dim = 3.1215\n",
      "[step 226] mean entropy per-dim = 3.1142\n",
      "[step 227] mean entropy per-dim = 3.1129\n",
      "[step 228] mean entropy per-dim = 3.1130\n",
      "[step 229] mean entropy per-dim = 3.1157\n",
      "[step 230] mean entropy per-dim = 3.1130\n",
      "[step 231] mean entropy per-dim = 3.1156\n",
      "[step 232] mean entropy per-dim = 3.1138\n",
      "[step 233] mean entropy per-dim = 3.1122\n",
      "[step 234] mean entropy per-dim = 3.1124\n",
      "[step 235] mean entropy per-dim = 3.1171\n",
      "[step 236] mean entropy per-dim = 3.1117\n",
      "[step 237] mean entropy per-dim = 3.1144\n",
      "[step 238] mean entropy per-dim = 3.1101\n",
      "[step 239] mean entropy per-dim = 3.1134\n",
      "[step 240] mean entropy per-dim = 3.1087\n",
      "train_step no.= 1241\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 8.724\n",
      "best_step_index: [5, 23]\n",
      "collect_traj: [[-1.         -1.         -0.5       ]\n",
      " [-1.         -0.9        -0.5       ]\n",
      " [-1.         -0.9        -0.39999998]\n",
      " [-0.9        -1.         -0.29999998]\n",
      " [-0.9        -1.1        -0.19999999]\n",
      " [-0.8        -1.1        -0.29999998]\n",
      " [-0.7        -1.1        -0.29999998]\n",
      " [-0.7        -1.         -0.39999998]\n",
      " [-0.59999996 -0.9        -0.29999998]\n",
      " [-0.5        -1.         -0.39999998]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.4        -1.6       ]\n",
      " [-1.5        -1.3        -1.5       ]\n",
      " [-1.5        -1.4        -1.6       ]\n",
      " [-1.4        -1.3        -1.6       ]\n",
      " [-1.4        -1.2        -1.6       ]\n",
      " [-1.4        -1.3        -1.7       ]\n",
      " [-1.3        -1.3        -1.8       ]\n",
      " [-1.2        -1.2        -1.7       ]\n",
      " [-1.3        -1.2        -1.6       ]\n",
      " [-1.2        -1.1        -1.5       ]\n",
      " [-1.3        -1.1        -1.6       ]\n",
      " [-1.2        -1.1        -1.7       ]\n",
      " [-1.2        -1.         -1.7       ]\n",
      " [-1.3        -1.1        -1.6       ]\n",
      " [-1.3        -1.1        -1.6       ]\n",
      " [-1.4        -1.1        -1.5       ]\n",
      " [-1.3        -1.1        -1.6       ]\n",
      " [-1.4        -1.         -1.7       ]\n",
      " [-1.5        -1.1        -1.8       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 241] mean entropy per-dim = 3.1109\n",
      "[step 242] mean entropy per-dim = 3.1142\n",
      "[step 243] mean entropy per-dim = 3.1093\n",
      "[step 244] mean entropy per-dim = 3.1055\n",
      "[step 245] mean entropy per-dim = 3.1148\n",
      "[step 246] mean entropy per-dim = 3.1066\n",
      "[step 247] mean entropy per-dim = 3.1111\n",
      "[step 248] mean entropy per-dim = 3.1080\n",
      "[step 249] mean entropy per-dim = 3.1054\n",
      "final reward before udpate: 12.2962\n",
      "final reward after udpate: 12.6817\n",
      "updated final_solution= [ 0.90000004 -0.8        -0.59999996]\n",
      "[step 250] mean entropy per-dim = 3.1163\n",
      "[step 251] mean entropy per-dim = 3.1116\n",
      "[step 252] mean entropy per-dim = 3.1033\n",
      "[step 253] mean entropy per-dim = 3.1056\n",
      "[step 254] mean entropy per-dim = 3.1051\n",
      "[step 255] mean entropy per-dim = 3.1045\n",
      "[step 256] mean entropy per-dim = 3.1130\n",
      "[step 257] mean entropy per-dim = 3.1108\n",
      "[step 258] mean entropy per-dim = 3.1106\n",
      "[step 259] mean entropy per-dim = 3.1088\n",
      "[step 260] mean entropy per-dim = 3.1103\n",
      "[step 261] mean entropy per-dim = 3.1067\n",
      "[step 262] mean entropy per-dim = 3.1069\n",
      "[step 263] mean entropy per-dim = 3.1070\n",
      "[step 264] mean entropy per-dim = 3.1084\n",
      "[step 265] mean entropy per-dim = 3.1026\n",
      "[step 266] mean entropy per-dim = 3.1051\n",
      "[step 267] mean entropy per-dim = 3.1109\n",
      "[step 268] mean entropy per-dim = 3.1063\n",
      "[step 269] mean entropy per-dim = 3.1040\n",
      "[step 270] mean entropy per-dim = 3.1045\n",
      "train_step no.= 1271\n",
      "best_solution of this generation= [-0.9 -0.8 -0.8]\n",
      "best step reward= 11.678 11.6777\n",
      "avg step reward= 7.689\n",
      "best_step_index: [17, 23]\n",
      "collect_traj: [[-1.1        -0.9        -1.8       ]\n",
      " [-1.         -0.8        -1.7       ]\n",
      " [-0.9        -0.7        -1.7       ]\n",
      " [-0.9        -0.59999996 -1.7       ]\n",
      " [-0.8        -0.59999996 -1.8       ]\n",
      " [-0.8        -0.59999996 -1.8       ]\n",
      " [-0.9        -0.7        -1.9       ]\n",
      " [-0.8        -0.59999996 -1.8       ]\n",
      " [-0.7        -0.59999996 -1.8       ]\n",
      " [-0.8        -0.59999996 -1.9       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.4        -1.5       ]\n",
      " [-1.5        -1.3        -1.4       ]\n",
      " [-1.4        -1.3        -1.3       ]\n",
      " [-1.3        -1.2        -1.3       ]\n",
      " [-1.3        -1.1        -1.3       ]\n",
      " [-1.2        -1.         -1.4       ]\n",
      " [-1.1        -1.         -1.5       ]\n",
      " [-1.2        -0.9        -1.4       ]\n",
      " [-1.1        -0.8        -1.4       ]\n",
      " [-1.         -0.7        -1.4       ]\n",
      " [-1.         -0.59999996 -1.5       ]\n",
      " [-0.9        -0.5        -1.5       ]\n",
      " [-0.9        -0.39999998 -1.6       ]\n",
      " [-0.9        -0.5        -1.7       ]\n",
      " [-0.8        -0.5        -1.8       ]\n",
      " [-0.7        -0.59999996 -1.9       ]\n",
      " [-0.8        -0.59999996 -1.8       ]\n",
      " [-0.7        -0.59999996 -1.9       ]\n",
      " [-0.8        -0.59999996 -1.9       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 271] mean entropy per-dim = 3.1051\n",
      "[step 272] mean entropy per-dim = 3.1040\n",
      "[step 273] mean entropy per-dim = 3.1109\n",
      "[step 274] mean entropy per-dim = 3.1100\n",
      "[step 275] mean entropy per-dim = 3.1020\n",
      "[step 276] mean entropy per-dim = 3.1034\n",
      "[step 277] mean entropy per-dim = 3.1035\n",
      "[step 278] mean entropy per-dim = 3.1018\n",
      "[step 279] mean entropy per-dim = 3.0997\n",
      "[step 280] mean entropy per-dim = 3.1017\n",
      "[step 281] mean entropy per-dim = 3.1055\n",
      "[step 282] mean entropy per-dim = 3.1015\n",
      "[step 283] mean entropy per-dim = 3.1079\n",
      "[step 284] mean entropy per-dim = 3.1109\n",
      "[step 285] mean entropy per-dim = 3.1100\n",
      "[step 286] mean entropy per-dim = 3.1044\n",
      "[step 287] mean entropy per-dim = 3.1081\n",
      "[step 288] mean entropy per-dim = 3.1042\n",
      "[step 289] mean entropy per-dim = 3.1072\n",
      "[step 290] mean entropy per-dim = 3.1012\n",
      "[step 291] mean entropy per-dim = 3.1037\n",
      "[step 292] mean entropy per-dim = 3.1038\n",
      "[step 293] mean entropy per-dim = 3.1032\n",
      "[step 294] mean entropy per-dim = 3.1041\n",
      "[step 295] mean entropy per-dim = 3.1081\n",
      "[step 296] mean entropy per-dim = 3.1027\n",
      "[step 297] mean entropy per-dim = 3.1038\n",
      "[step 298] mean entropy per-dim = 3.1014\n",
      "[step 299] mean entropy per-dim = 3.1059\n",
      "[step 300] mean entropy per-dim = 3.1092\n",
      "train_step no.= 1301\n",
      "best_solution of this generation= [-0.8 -0.8 -0.8]\n",
      "best step reward= 11.695 11.6952\n",
      "avg step reward= 9.137\n",
      "best_step_index: [6, 26]\n",
      "collect_traj: [[-0.59999996 -0.9        -0.8       ]\n",
      " [-0.5        -1.         -0.7       ]\n",
      " [-0.59999996 -0.9        -0.8       ]\n",
      " [-0.5        -0.9        -0.8       ]\n",
      " [-0.39999998 -0.8        -0.8       ]\n",
      " [-0.39999998 -0.8        -0.9       ]\n",
      " [-0.29999998 -0.7        -0.9       ]\n",
      " [-0.19999999 -0.7        -1.        ]\n",
      " [-0.09999998 -0.59999996 -1.        ]\n",
      " [-0.19999999 -0.59999996 -0.9       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.6        -1.4        -1.6       ]\n",
      " [-1.7        -1.5        -1.5       ]\n",
      " [-1.8        -1.5        -1.4       ]\n",
      " [-1.7        -1.5        -1.3       ]\n",
      " [-1.7        -1.4        -1.2       ]\n",
      " [-1.6        -1.3        -1.1       ]\n",
      " [-1.6        -1.2        -1.2       ]\n",
      " [-1.5        -1.1        -1.3       ]\n",
      " [-1.6        -1.2        -1.2       ]\n",
      " [-1.5        -1.2        -1.1       ]\n",
      " [-1.5        -1.1        -1.        ]\n",
      " [-1.4        -1.         -1.        ]\n",
      " [-1.4        -0.9        -1.1       ]\n",
      " [-1.3        -0.8        -1.        ]\n",
      " [-1.2        -0.9        -1.        ]\n",
      " [-1.1        -0.8        -1.1       ]\n",
      " [-1.1        -0.9        -1.2       ]\n",
      " [-1.1        -0.8        -1.1       ]\n",
      " [-1.         -0.8        -1.1       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 301] mean entropy per-dim = 3.1009\n",
      "[step 302] mean entropy per-dim = 3.1053\n",
      "[step 303] mean entropy per-dim = 3.1047\n",
      "[step 304] mean entropy per-dim = 3.1103\n",
      "[step 305] mean entropy per-dim = 3.1030\n",
      "[step 306] mean entropy per-dim = 3.1048\n",
      "[step 307] mean entropy per-dim = 3.1029\n",
      "[step 308] mean entropy per-dim = 3.1072\n",
      "[step 309] mean entropy per-dim = 3.0965\n",
      "[step 310] mean entropy per-dim = 3.0983\n",
      "[step 311] mean entropy per-dim = 3.1025\n",
      "[step 312] mean entropy per-dim = 3.1035\n",
      "[step 313] mean entropy per-dim = 3.1025\n",
      "[step 314] mean entropy per-dim = 3.1009\n",
      "[step 315] mean entropy per-dim = 3.1051\n",
      "[step 316] mean entropy per-dim = 3.1039\n",
      "[step 317] mean entropy per-dim = 3.1061\n",
      "[step 318] mean entropy per-dim = 3.1020\n",
      "[step 319] mean entropy per-dim = 3.1031\n",
      "[step 320] mean entropy per-dim = 3.1067\n",
      "[step 321] mean entropy per-dim = 3.1016\n",
      "[step 322] mean entropy per-dim = 3.1057\n",
      "[step 323] mean entropy per-dim = 3.1100\n",
      "[step 324] mean entropy per-dim = 3.1079\n",
      "[step 325] mean entropy per-dim = 3.1082\n",
      "[step 326] mean entropy per-dim = 3.1120\n",
      "[step 327] mean entropy per-dim = 3.1055\n",
      "[step 328] mean entropy per-dim = 3.0979\n",
      "[step 329] mean entropy per-dim = 3.1097\n",
      "[step 330] mean entropy per-dim = 3.1009\n",
      "train_step no.= 1331\n",
      "best_solution of this generation= [-0.9 -0.8 -0.8]\n",
      "best step reward= 11.678 11.6777\n",
      "avg step reward= 8.816\n",
      "best_step_index: [11, 24]\n",
      "collect_traj: [[-1.         -0.39999998 -0.7       ]\n",
      " [-1.         -0.5        -0.59999996]\n",
      " [-1.         -0.39999998 -0.59999996]\n",
      " [-1.         -0.29999998 -0.59999996]\n",
      " [-1.1        -0.19999999 -0.7       ]\n",
      " [-1.1        -0.19999999 -0.59999996]\n",
      " [-1.         -0.19999999 -0.59999996]\n",
      " [-1.1        -0.29999998 -0.5       ]\n",
      " [-1.         -0.29999998 -0.39999998]\n",
      " [-1.         -0.19999999 -0.29999998]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.4        -1.6       ]\n",
      " [-1.5        -1.4        -1.6       ]\n",
      " [-1.4        -1.3        -1.5       ]\n",
      " [-1.3        -1.3        -1.4       ]\n",
      " [-1.2        -1.3        -1.4       ]\n",
      " [-1.2        -1.4        -1.3       ]\n",
      " [-1.1        -1.4        -1.4       ]\n",
      " [-1.1        -1.3        -1.3       ]\n",
      " [-1.1        -1.3        -1.3       ]\n",
      " [-1.         -1.2        -1.3       ]\n",
      " [-1.         -1.1        -1.2       ]\n",
      " [-1.1        -1.2        -1.2       ]\n",
      " [-1.1        -1.1        -1.1       ]\n",
      " [-1.         -1.         -1.1       ]\n",
      " [-0.9        -1.1        -1.1       ]\n",
      " [-0.8        -1.         -1.        ]\n",
      " [-0.9        -0.9        -0.9       ]\n",
      " [-0.9        -1.         -0.8       ]\n",
      " [-0.9        -0.9        -0.8       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 331] mean entropy per-dim = 3.1026\n",
      "[step 332] mean entropy per-dim = 3.1018\n",
      "[step 333] mean entropy per-dim = 3.0985\n",
      "[step 334] mean entropy per-dim = 3.1042\n",
      "[step 335] mean entropy per-dim = 3.0987\n",
      "[step 336] mean entropy per-dim = 3.1007\n",
      "[step 337] mean entropy per-dim = 3.1101\n",
      "[step 338] mean entropy per-dim = 3.1122\n",
      "[step 339] mean entropy per-dim = 3.1029\n",
      "[step 340] mean entropy per-dim = 3.1057\n",
      "[step 341] mean entropy per-dim = 3.1000\n",
      "[step 342] mean entropy per-dim = 3.1030\n",
      "[step 343] mean entropy per-dim = 3.1017\n",
      "[step 344] mean entropy per-dim = 3.1037\n",
      "[step 345] mean entropy per-dim = 3.0960\n",
      "[step 346] mean entropy per-dim = 3.0996\n",
      "[step 347] mean entropy per-dim = 3.1003\n",
      "[step 348] mean entropy per-dim = 3.1011\n",
      "[step 349] mean entropy per-dim = 3.1023\n",
      "[step 350] mean entropy per-dim = 3.1028\n",
      "[step 351] mean entropy per-dim = 3.1081\n",
      "[step 352] mean entropy per-dim = 3.0980\n",
      "[step 353] mean entropy per-dim = 3.1088\n",
      "[step 354] mean entropy per-dim = 3.1056\n",
      "[step 355] mean entropy per-dim = 3.0954\n",
      "[step 356] mean entropy per-dim = 3.0931\n",
      "[step 357] mean entropy per-dim = 3.1071\n",
      "[step 358] mean entropy per-dim = 3.0996\n",
      "[step 359] mean entropy per-dim = 3.0953\n",
      "[step 360] mean entropy per-dim = 3.0998\n",
      "train_step no.= 1361\n",
      "best_solution of this generation= [-0.59999996 -0.7         0.3       ]\n",
      "best step reward= 11.72 11.7202\n",
      "avg step reward= 8.796\n",
      "best_step_index: [11, 9]\n",
      "collect_traj: [[-0.5        -0.8        -1.2       ]\n",
      " [-0.59999996 -0.8        -1.2       ]\n",
      " [-0.59999996 -0.8        -1.3       ]\n",
      " [-0.5        -0.7        -1.2       ]\n",
      " [-0.5        -0.7        -1.1       ]\n",
      " [-0.39999998 -0.7        -1.        ]\n",
      " [-0.39999998 -0.59999996 -0.9       ]\n",
      " [-0.29999998 -0.59999996 -0.9       ]\n",
      " [-0.19999999 -0.5        -0.9       ]\n",
      " [-0.29999998 -0.39999998 -0.8       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.4        -1.4       ]\n",
      " [-1.4        -1.3        -1.3       ]\n",
      " [-1.4        -1.3        -1.4       ]\n",
      " [-1.3        -1.3        -1.5       ]\n",
      " [-1.2        -1.2        -1.4       ]\n",
      " [-1.1        -1.1        -1.3       ]\n",
      " [-1.1        -1.         -1.2       ]\n",
      " [-1.1        -0.9        -1.3       ]\n",
      " [-1.1        -0.8        -1.4       ]\n",
      " [-1.2        -0.8        -1.3       ]\n",
      " [-1.3        -0.9        -1.2       ]\n",
      " [-1.3        -0.9        -1.2       ]\n",
      " [-1.3        -1.         -1.1       ]\n",
      " [-1.3        -1.         -1.2       ]\n",
      " [-1.3        -0.9        -1.3       ]\n",
      " [-1.2        -0.9        -1.3       ]\n",
      " [-1.1        -0.8        -1.4       ]\n",
      " [-1.1        -0.7        -1.3       ]\n",
      " [-1.         -0.7        -1.2       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 361] mean entropy per-dim = 3.0948\n",
      "[step 362] mean entropy per-dim = 3.0977\n",
      "[step 363] mean entropy per-dim = 3.1025\n",
      "[step 364] mean entropy per-dim = 3.0962\n",
      "[step 365] mean entropy per-dim = 3.1000\n",
      "[step 366] mean entropy per-dim = 3.0984\n",
      "[step 367] mean entropy per-dim = 3.1021\n",
      "[step 368] mean entropy per-dim = 3.0974\n",
      "[step 369] mean entropy per-dim = 3.1088\n",
      "[step 370] mean entropy per-dim = 3.0960\n",
      "[step 371] mean entropy per-dim = 3.0952\n",
      "[step 372] mean entropy per-dim = 3.0952\n",
      "[step 373] mean entropy per-dim = 3.1001\n",
      "[step 374] mean entropy per-dim = 3.0983\n",
      "[step 375] mean entropy per-dim = 3.1029\n",
      "[step 376] mean entropy per-dim = 3.0979\n",
      "[step 377] mean entropy per-dim = 3.0924\n",
      "[step 378] mean entropy per-dim = 3.0958\n",
      "[step 379] mean entropy per-dim = 3.1009\n",
      "[step 380] mean entropy per-dim = 3.0960\n",
      "[step 381] mean entropy per-dim = 3.0965\n",
      "[step 382] mean entropy per-dim = 3.0918\n",
      "[step 383] mean entropy per-dim = 3.0987\n",
      "[step 384] mean entropy per-dim = 3.0911\n",
      "[step 385] mean entropy per-dim = 3.0902\n",
      "[step 386] mean entropy per-dim = 3.0986\n",
      "[step 387] mean entropy per-dim = 3.0945\n",
      "[step 388] mean entropy per-dim = 3.0918\n",
      "[step 389] mean entropy per-dim = 3.0902\n",
      "[step 390] mean entropy per-dim = 3.0992\n",
      "train_step no.= 1391\n",
      "best_solution of this generation= [-0.9 -0.7 -0.9]\n",
      "best step reward= 11.635 11.6347\n",
      "avg step reward= 8.58\n",
      "best_step_index: [10, 4]\n",
      "collect_traj: [[-2.9999998e-01 -8.9999998e-01 -8.0000001e-01]\n",
      " [-3.9999998e-01 -8.0000001e-01 -8.0000001e-01]\n",
      " [-2.9999998e-01 -6.9999999e-01 -6.9999999e-01]\n",
      " [-1.9999999e-01 -5.9999996e-01 -5.9999996e-01]\n",
      " [-1.9999999e-01 -5.0000000e-01 -6.9999999e-01]\n",
      " [-1.9999999e-01 -3.9999998e-01 -8.0000001e-01]\n",
      " [-9.9999979e-02 -2.9999998e-01 -8.9999998e-01]\n",
      " [ 2.2351742e-08 -1.9999999e-01 -1.0000000e+00]\n",
      " [-9.9999979e-02 -9.9999979e-02 -1.1000000e+00]\n",
      " [-9.9999979e-02  2.2351742e-08 -1.0000000e+00]\n",
      " [-1.5000000e+00 -1.5000000e+00 -1.5000000e+00]\n",
      " [-1.6000000e+00 -1.4000000e+00 -1.5000000e+00]\n",
      " [-1.5000000e+00 -1.3000000e+00 -1.4000000e+00]\n",
      " [-1.6000000e+00 -1.2000000e+00 -1.3000000e+00]\n",
      " [-1.5000000e+00 -1.3000000e+00 -1.2000000e+00]\n",
      " [-1.5000000e+00 -1.3000000e+00 -1.1000000e+00]\n",
      " [-1.5000000e+00 -1.2000000e+00 -1.0000000e+00]\n",
      " [-1.5000000e+00 -1.1000000e+00 -8.9999998e-01]\n",
      " [-1.4000000e+00 -1.1000000e+00 -8.0000001e-01]\n",
      " [-1.3000000e+00 -1.1000000e+00 -6.9999999e-01]\n",
      " [-1.3000000e+00 -1.0000000e+00 -8.0000001e-01]\n",
      " [-1.2000000e+00 -1.0000000e+00 -6.9999999e-01]\n",
      " [-1.2000000e+00 -8.9999998e-01 -5.9999996e-01]\n",
      " [-1.3000000e+00 -8.9999998e-01 -5.9999996e-01]\n",
      " [-1.3000000e+00 -1.0000000e+00 -5.9999996e-01]\n",
      " [-1.3000000e+00 -8.9999998e-01 -6.9999999e-01]\n",
      " [-1.3000000e+00 -8.0000001e-01 -6.9999999e-01]\n",
      " [-1.3000000e+00 -6.9999999e-01 -8.0000001e-01]\n",
      " [-1.2000000e+00 -5.9999996e-01 -8.0000001e-01]\n",
      " [-1.2000000e+00 -5.9999996e-01 -8.0000001e-01]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 391] mean entropy per-dim = 3.0952\n",
      "[step 392] mean entropy per-dim = 3.1026\n",
      "[step 393] mean entropy per-dim = 3.0916\n",
      "[step 394] mean entropy per-dim = 3.0994\n",
      "[step 395] mean entropy per-dim = 3.1011\n",
      "[step 396] mean entropy per-dim = 3.0921\n",
      "[step 397] mean entropy per-dim = 3.0921\n",
      "[step 398] mean entropy per-dim = 3.0855\n",
      "[step 399] mean entropy per-dim = 3.0917\n",
      "[step 400] mean entropy per-dim = 3.0993\n",
      "[step 401] mean entropy per-dim = 3.0913\n",
      "[step 402] mean entropy per-dim = 3.0887\n",
      "[step 403] mean entropy per-dim = 3.0963\n",
      "[step 404] mean entropy per-dim = 3.0954\n",
      "[step 405] mean entropy per-dim = 3.0953\n",
      "[step 406] mean entropy per-dim = 3.0906\n",
      "[step 407] mean entropy per-dim = 3.0863\n",
      "[step 408] mean entropy per-dim = 3.0989\n",
      "[step 409] mean entropy per-dim = 3.0900\n",
      "[step 410] mean entropy per-dim = 3.0918\n",
      "[step 411] mean entropy per-dim = 3.0876\n",
      "[step 412] mean entropy per-dim = 3.0847\n",
      "[step 413] mean entropy per-dim = 3.0842\n",
      "[step 414] mean entropy per-dim = 3.0909\n",
      "[step 415] mean entropy per-dim = 3.0853\n",
      "[step 416] mean entropy per-dim = 3.0942\n",
      "[step 417] mean entropy per-dim = 3.0948\n",
      "[step 418] mean entropy per-dim = 3.1007\n",
      "[step 419] mean entropy per-dim = 3.0942\n",
      "[step 420] mean entropy per-dim = 3.1000\n",
      "train_step no.= 1421\n",
      "best_solution of this generation= [-0.9 -0.8 -0.9]\n",
      "best step reward= 11.66 11.6602\n",
      "avg step reward= 8.85\n",
      "best_step_index: [8, 21]\n",
      "collect_traj: [[-1.         -1.         -0.7       ]\n",
      " [-0.9        -1.         -0.59999996]\n",
      " [-0.8        -1.1        -0.5       ]\n",
      " [-0.8        -1.2        -0.5       ]\n",
      " [-0.8        -1.1        -0.59999996]\n",
      " [-0.7        -1.1        -0.5       ]\n",
      " [-0.7        -1.         -0.39999998]\n",
      " [-0.7        -0.9        -0.5       ]\n",
      " [-0.7        -0.9        -0.39999998]\n",
      " [-0.59999996 -0.9        -0.39999998]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.5        -1.4        -1.4       ]\n",
      " [-1.6        -1.3        -1.4       ]\n",
      " [-1.7        -1.3        -1.3       ]\n",
      " [-1.7        -1.2        -1.2       ]\n",
      " [-1.7        -1.2        -1.1       ]\n",
      " [-1.6        -1.3        -1.        ]\n",
      " [-1.5        -1.2        -0.9       ]\n",
      " [-1.4        -1.3        -0.9       ]\n",
      " [-1.5        -1.3        -1.        ]\n",
      " [-1.5        -1.2        -0.9       ]\n",
      " [-1.4        -1.2        -0.8       ]\n",
      " [-1.3        -1.1        -0.9       ]\n",
      " [-1.4        -1.1        -0.8       ]\n",
      " [-1.5        -1.1        -0.7       ]\n",
      " [-1.6        -1.         -0.7       ]\n",
      " [-1.5        -0.9        -0.59999996]\n",
      " [-1.4        -0.9        -0.5       ]\n",
      " [-1.5        -0.9        -0.39999998]\n",
      " [-1.4        -0.9        -0.5       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 421] mean entropy per-dim = 3.0981\n",
      "[step 422] mean entropy per-dim = 3.0918\n",
      "[step 423] mean entropy per-dim = 3.0930\n",
      "[step 424] mean entropy per-dim = 3.0889\n",
      "[step 425] mean entropy per-dim = 3.0931\n",
      "[step 426] mean entropy per-dim = 3.0913\n",
      "[step 427] mean entropy per-dim = 3.0910\n",
      "[step 428] mean entropy per-dim = 3.0909\n",
      "[step 429] mean entropy per-dim = 3.0883\n",
      "[step 430] mean entropy per-dim = 3.0883\n",
      "[step 431] mean entropy per-dim = 3.0922\n",
      "[step 432] mean entropy per-dim = 3.0952\n",
      "[step 433] mean entropy per-dim = 3.0898\n",
      "[step 434] mean entropy per-dim = 3.0940\n",
      "[step 435] mean entropy per-dim = 3.0933\n",
      "[step 436] mean entropy per-dim = 3.0953\n",
      "[step 437] mean entropy per-dim = 3.0898\n",
      "[step 438] mean entropy per-dim = 3.0925\n",
      "[step 439] mean entropy per-dim = 3.0966\n",
      "[step 440] mean entropy per-dim = 3.0912\n",
      "[step 441] mean entropy per-dim = 3.0906\n",
      "[step 442] mean entropy per-dim = 3.0868\n",
      "[step 443] mean entropy per-dim = 3.0921\n",
      "[step 444] mean entropy per-dim = 3.0919\n",
      "[step 445] mean entropy per-dim = 3.0880\n",
      "[step 446] mean entropy per-dim = 3.0872\n",
      "[step 447] mean entropy per-dim = 3.0983\n",
      "[step 448] mean entropy per-dim = 3.0874\n",
      "[step 449] mean entropy per-dim = 3.0880\n",
      "[step 450] mean entropy per-dim = 3.0825\n",
      "train_step no.= 1451\n",
      "best_solution of this generation= [ 0.40000004 -0.29999998 -0.9       ]\n",
      "best step reward= 11.732 11.732201\n",
      "avg step reward= 9.093\n",
      "best_step_index: [16, 9]\n",
      "collect_traj: [[-2.99999982e-01 -6.99999988e-01 -1.20000005e+00]\n",
      " [-1.99999988e-01 -6.99999988e-01 -1.10000002e+00]\n",
      " [-9.99999791e-02 -5.99999964e-01 -1.00000000e+00]\n",
      " [-1.99999988e-01 -5.00000000e-01 -1.10000002e+00]\n",
      " [-9.99999791e-02 -3.99999976e-01 -1.00000000e+00]\n",
      " [ 2.23517418e-08 -2.99999982e-01 -8.99999976e-01]\n",
      " [ 2.23517418e-08 -2.99999982e-01 -1.00000000e+00]\n",
      " [ 1.00000024e-01 -1.99999988e-01 -8.99999976e-01]\n",
      " [ 2.00000018e-01 -2.99999982e-01 -8.99999976e-01]\n",
      " [ 1.00000024e-01 -1.99999988e-01 -8.00000012e-01]\n",
      " [-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.50000000e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.29999995e+00 -1.50000000e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.60000002e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.60000002e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.60000002e+00 -1.39999998e+00]\n",
      " [-1.39999998e+00 -1.50000000e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.60000002e+00 -1.29999995e+00]\n",
      " [-1.29999995e+00 -1.70000005e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.60000002e+00 -1.50000000e+00]\n",
      " [-1.20000005e+00 -1.60000002e+00 -1.39999998e+00]\n",
      " [-1.10000002e+00 -1.60000002e+00 -1.29999995e+00]\n",
      " [-1.10000002e+00 -1.50000000e+00 -1.29999995e+00]\n",
      " [-1.00000000e+00 -1.60000002e+00 -1.20000005e+00]\n",
      " [-1.00000000e+00 -1.50000000e+00 -1.20000005e+00]\n",
      " [-8.99999976e-01 -1.50000000e+00 -1.20000005e+00]\n",
      " [-8.00000012e-01 -1.60000002e+00 -1.20000005e+00]\n",
      " [-8.00000012e-01 -1.60000002e+00 -1.10000002e+00]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  8.00000012e-01]\n",
      " [-9.99999791e-02  5.00000000e-01  9.00000036e-01]\n",
      " [-1.99999988e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-2.99999982e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-3.99999976e-01  2.00000018e-01  1.20000005e+00]\n",
      " [-5.00000000e-01  1.00000024e-01  1.30000007e+00]\n",
      " [-5.99999964e-01  2.23517418e-08  1.40000010e+00]]\n",
      "[step 451] mean entropy per-dim = 3.0832\n",
      "[step 452] mean entropy per-dim = 3.0844\n",
      "[step 453] mean entropy per-dim = 3.0905\n",
      "[step 454] mean entropy per-dim = 3.0783\n",
      "[step 455] mean entropy per-dim = 3.0856\n",
      "[step 456] mean entropy per-dim = 3.0825\n",
      "[step 457] mean entropy per-dim = 3.0888\n",
      "[step 458] mean entropy per-dim = 3.0860\n",
      "[step 459] mean entropy per-dim = 3.0814\n",
      "[step 460] mean entropy per-dim = 3.0736\n",
      "[step 461] mean entropy per-dim = 3.0757\n",
      "[step 462] mean entropy per-dim = 3.0806\n",
      "[step 463] mean entropy per-dim = 3.0776\n",
      "[step 464] mean entropy per-dim = 3.0806\n",
      "[step 465] mean entropy per-dim = 3.0806\n",
      "[step 466] mean entropy per-dim = 3.0795\n",
      "[step 467] mean entropy per-dim = 3.0788\n",
      "[step 468] mean entropy per-dim = 3.0816\n",
      "[step 469] mean entropy per-dim = 3.0749\n",
      "[step 470] mean entropy per-dim = 3.0802\n",
      "[step 471] mean entropy per-dim = 3.0759\n",
      "[step 472] mean entropy per-dim = 3.0808\n",
      "[step 473] mean entropy per-dim = 3.0834\n",
      "[step 474] mean entropy per-dim = 3.0858\n",
      "[step 475] mean entropy per-dim = 3.0787\n",
      "[step 476] mean entropy per-dim = 3.0727\n",
      "[step 477] mean entropy per-dim = 3.0762\n",
      "[step 478] mean entropy per-dim = 3.0803\n",
      "[step 479] mean entropy per-dim = 3.0727\n",
      "[step 480] mean entropy per-dim = 3.0820\n",
      "train_step no.= 1481\n",
      "best_solution of this generation= [-0.9  0.3 -1. ]\n",
      "best step reward= 11.706 11.7058\n",
      "avg step reward= 9.172\n",
      "best_step_index: [15, 9]\n",
      "collect_traj: [[-0.9        -1.         -1.        ]\n",
      " [-0.8        -0.9        -1.        ]\n",
      " [-0.7        -0.9        -1.        ]\n",
      " [-0.8        -0.9        -0.9       ]\n",
      " [-0.9        -0.8        -1.        ]\n",
      " [-0.8        -0.7        -1.        ]\n",
      " [-0.7        -0.59999996 -1.1       ]\n",
      " [-0.8        -0.59999996 -1.1       ]\n",
      " [-0.9        -0.59999996 -1.2       ]\n",
      " [-0.9        -0.5        -1.2       ]\n",
      " [-1.5        -1.5        -1.5       ]\n",
      " [-1.4        -1.6        -1.5       ]\n",
      " [-1.5        -1.6        -1.4       ]\n",
      " [-1.4        -1.7        -1.5       ]\n",
      " [-1.5        -1.6        -1.4       ]\n",
      " [-1.5        -1.5        -1.4       ]\n",
      " [-1.5        -1.5        -1.3       ]\n",
      " [-1.5        -1.4        -1.2       ]\n",
      " [-1.5        -1.3        -1.1       ]\n",
      " [-1.4        -1.3        -1.        ]\n",
      " [-1.5        -1.3        -0.9       ]\n",
      " [-1.5        -1.2        -0.8       ]\n",
      " [-1.4        -1.2        -0.8       ]\n",
      " [-1.3        -1.1        -0.7       ]\n",
      " [-1.3        -1.         -0.7       ]\n",
      " [-1.3        -1.         -0.8       ]\n",
      " [-1.2        -1.         -0.9       ]\n",
      " [-1.2        -1.         -0.8       ]\n",
      " [-1.1        -0.9        -0.8       ]\n",
      " [-1.         -0.9        -0.7       ]]\n",
      "test_traj [[-1.50000000e+00 -1.50000000e+00 -1.50000000e+00]\n",
      " [-1.39999998e+00 -1.39999998e+00 -1.39999998e+00]\n",
      " [-1.29999995e+00 -1.29999995e+00 -1.29999995e+00]\n",
      " [-1.20000005e+00 -1.20000005e+00 -1.20000005e+00]\n",
      " [-1.10000002e+00 -1.10000002e+00 -1.10000002e+00]\n",
      " [-1.00000000e+00 -1.00000000e+00 -1.00000000e+00]\n",
      " [-8.99999976e-01 -8.99999976e-01 -8.99999976e-01]\n",
      " [-8.00000012e-01 -8.00000012e-01 -8.00000012e-01]\n",
      " [-6.99999988e-01 -6.99999988e-01 -6.99999988e-01]\n",
      " [-5.99999964e-01 -5.99999964e-01 -5.99999964e-01]\n",
      " [-5.00000000e-01 -5.00000000e-01 -5.00000000e-01]\n",
      " [-3.99999976e-01 -3.99999976e-01 -3.99999976e-01]\n",
      " [-2.99999982e-01 -2.99999982e-01 -2.99999982e-01]\n",
      " [-1.99999988e-01 -1.99999988e-01 -1.99999988e-01]\n",
      " [-9.99999791e-02 -9.99999791e-02 -9.99999791e-02]\n",
      " [ 2.23517418e-08  2.23517418e-08  2.23517418e-08]\n",
      " [ 1.00000024e-01  1.00000024e-01  1.00000024e-01]\n",
      " [ 2.00000018e-01  2.00000018e-01  2.00000018e-01]\n",
      " [ 3.00000012e-01  3.00000012e-01  3.00000012e-01]\n",
      " [ 4.00000036e-01  4.00000036e-01  4.00000036e-01]\n",
      " [ 3.00000012e-01  5.00000000e-01  5.00000000e-01]\n",
      " [ 2.00000018e-01  6.00000024e-01  6.00000024e-01]\n",
      " [ 1.00000024e-01  7.00000048e-01  7.00000048e-01]\n",
      " [ 2.23517418e-08  6.00000024e-01  6.00000024e-01]\n",
      " [-9.99999791e-02  7.00000048e-01  7.00000048e-01]\n",
      " [-1.99999988e-01  6.00000024e-01  8.00000012e-01]\n",
      " [-2.99999982e-01  5.00000000e-01  9.00000036e-01]\n",
      " [-3.99999976e-01  4.00000036e-01  1.00000000e+00]\n",
      " [-5.00000000e-01  3.00000012e-01  1.10000002e+00]\n",
      " [-5.99999964e-01  2.00000018e-01  1.20000005e+00]]\n",
      "[step 481] mean entropy per-dim = 3.0763\n",
      "[step 482] mean entropy per-dim = 3.0779\n",
      "[step 483] mean entropy per-dim = 3.0771\n",
      "[step 484] mean entropy per-dim = 3.0818\n",
      "[step 485] mean entropy per-dim = 3.0806\n",
      "[step 486] mean entropy per-dim = 3.0801\n",
      "[step 487] mean entropy per-dim = 3.0771\n",
      "[step 488] mean entropy per-dim = 3.0823\n",
      "[step 489] mean entropy per-dim = 3.0810\n",
      "[step 490] mean entropy per-dim = 3.0762\n",
      "[step 491] mean entropy per-dim = 3.0764\n",
      "[step 492] mean entropy per-dim = 3.0668\n",
      "[step 493] mean entropy per-dim = 3.0689\n",
      "[step 494] mean entropy per-dim = 3.0789\n",
      "[step 495] mean entropy per-dim = 3.0770\n",
      "[step 496] mean entropy per-dim = 3.0786\n",
      "[step 497] mean entropy per-dim = 3.0826\n",
      "[step 498] mean entropy per-dim = 3.0759\n",
      "[step 499] mean entropy per-dim = 3.0843\n",
      "final_solution= [ 0.90000004 -0.8        -0.59999996] final_reward= 12.6817\n"
     ]
    }
   ],
   "source": [
    "##### - Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = 500 \n",
    "eval_intv = 30 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "#final_reward = -1000\n",
    "plot_intv = 500\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "\n",
    "        ######Entropy trace during “burn-in”, Watch the policy’s entropy during the very first few updates\n",
    "\n",
    "        ent = actions_distribution.entropy().numpy()   # shape = (batch_size, state_dim)\n",
    "        print(f\"[step {n:3d}] mean entropy per-dim = {ent.mean():.4f}\")\n",
    "        #######\n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('act_logits:', actions_distribution._logits.numpy()[0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print('collect_traj:', observations[0,:].numpy())\n",
    "        \n",
    "        #Test trajectory generated by mode of action_distribution in agent policy\n",
    "        test_buffer.clear()\n",
    "        test_driver.run(eval_env.reset())  #generate batches of trajectories with agent.collect_policy, and save them in replay_buffer\n",
    "        experience = test_buffer.gather_all()  #get all the stored items in replay_buffer\n",
    "        test_trajectory = experience.observation.numpy()[0,:]\n",
    "        print('test_traj', test_trajectory)\n",
    "        \n",
    "               \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "      )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874114e-b14c-49e2-98c7-ffa88292dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(np.array([-1.5,-1.5,-1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db9a8-3b23-4c17-a771-0141e4d94fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,0:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8e7cb-7dc6-46b0-a2a1-f58f3437a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rewards[:,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9d488-1fbd-45d7-8b95-c8d0eb9cd07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d038c-3404-4bd7-81b2-cc21da86dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,10],[3,4,13],[1,4,7],[1,9,102]]\n",
    "a = np.array(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d39e3-65d1-4782-88e2-58db29b28d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b892e-1fd7-43b6-878c-58a94929eb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
