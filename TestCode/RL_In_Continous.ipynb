{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc5246e-a3b8-413a-8a6e-19f419522636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 11:55:02.379370: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-07-09 11:55:02.379447: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SmallMONSTER): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from custom_normal_projection_network import NormalProjectionNetwork\n",
    "import os,gc\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "#To limit TensorFlow to a specific set of GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "#enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af6163-34e0-4d13-b7cb-4b372c2f659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the objective f to be maximized\n",
    "N = 4   #This the dimension number\n",
    "state_dim = N\n",
    "act_min = np.array([-20.0]*N)\n",
    "act_max = np.array([20.0]*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b47086-ca57-442e-9c58-8915b56a4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_multimodal_reward(x):\n",
    "    \"\"\"\n",
    "    x: np.array([x1, x2, x3, x4])\n",
    "    Domains:\n",
    "      x1 ∈ [-3, 5]\n",
    "      x2 ∈ [10, 14]\n",
    "      x3 ∈ [-8, -4]\n",
    "      x4 ∈ [0.5, 1.5]\n",
    "    \"\"\"\n",
    "    ##Only at [1.2, 12, -6, 1]\n",
    "    x1, x2, x3, x4 = x[0], x[1], x[2], x[3]\n",
    "    term1 = (x1-1.2)**2 + 10*np.cos(2*np.pi*(x1-1.2))\n",
    "    term2 = (x2-12)**2 + 8*np.cos(2*np.pi*(x2-12))\n",
    "    term3 = (x3+6)**2 + 6*np.cos(2*np.pi*(x3+6))\n",
    "    term4 = 30*(x4-1)**2 + 5*np.cos(2*np.pi*(x4-1))\n",
    "    reward = -(term1 + term2 + term3 + term4)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c6589-5c62-4669-907d-43733efe446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=np.array([-0.5,-0.5,-0.5,-0.5])\n",
    "m2=np.array([0.5,0.5,0.5,0.5])\n",
    "\n",
    "def f(x):\n",
    "    return -np.log(np.sum((x-m1)**2)+0.00001)-np.log(np.sum((x-m2)**2)+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4d1e3-b5f3-45ad-9cb8-c40b59dd8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set initial x-values for all three algorithms\n",
    "x0_reinforce = np.array([1.0, 0.0, -5.0, 0.2])\n",
    "final_reward   = -1e9\n",
    "final_solution = x0_reinforce.copy()    # so it always exists\n",
    "\n",
    "sub_episode_length = 50 #number of time_steps in a sub-episode. \n",
    "sub_episode_num_in_single_env = 6\n",
    "episode_length = sub_episode_length*sub_episode_num_in_single_env  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                      #each trajectory will be split to multiple episodes\n",
    "env_num = 3  #Number of parallel environments, each environment is used to generate an episode\n",
    "print('x0', x0_reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bb0d4-04ac-4df3-83f1-03e7a811eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#Set hyper-parameters for REINFORCE-OPT\n",
    "generation_num = 2000  #number of theta updates for REINFORCE-IP, also serves as the number\n",
    "                      #of generations for GA, and the number of iterations for particle swarm optimization\n",
    "\n",
    "disc_factor = 1.0\n",
    "alpha = 0.2 #regularization coefficient\n",
    "param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "sub_episode_num = int(env_num*(episode_length/sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "print(\"number of sub_episodes used for a single param update:\", sub_episode_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d30aa4-0aa4-4384-ac02-2da9f04a1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate*self.C/(self.C+step)\n",
    "lr = lr_schedule(initial_lr=0.001, C=50000)   \n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "opt = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "#opt = tf.keras.optimizers.legacy.Adam( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7211c5-25ad-4d0f-bb62-02bb0563ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed34e79-22ff-4c88-bb03-9aa4e0e097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(x):\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8552c58-f46e-43fb-a679-691729ecc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        '''The function to initialize an Env obj.\n",
    "        '''\n",
    "        #Specify the requirement for the value of action, (It is a 2d-array for this case)\n",
    "        #which is an argument of _step(self, action) that is later defined.\n",
    "        #tf_agents.specs.BoundedArraySpec is a class.\n",
    "        #_action_spec.check_array( arr ) returns true if arr conforms to the specification stored in _action_spec\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "                            shape=(state_dim,), dtype=np.float32, minimum=act_min, maximum=act_max, name='action') #a_t is an 2darray\n",
    "\n",
    "    \n",
    "        \n",
    "        #Specify the format requirement for observation (It is a 2d-array for this case), \n",
    "        #i.e. the observable part of S_t, and it is stored in self._state\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "                                 shape=(state_dim,), dtype=np.float32, name='observation') #default max and min is None\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)\n",
    "        #self.A = mat\n",
    "        self._episode_ended = False\n",
    "        #stop_threshold is a condition for terminating the process for looking for the solution\n",
    "        #self._stop_threshold = 0.01\n",
    "        self._step_counter = 0\n",
    "\n",
    "    def action_spec(self):\n",
    "        #return the format requirement for action\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        #return the format requirement for observation\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array(x0_reinforce,dtype=np.float32)  #initial state\n",
    "        self._episode_ended = False\n",
    "        self._step_counter = 0\n",
    "        \n",
    "        #Reward\n",
    "        initial_r = np.float32(0.0)\n",
    "        \n",
    "        #return ts.restart(observation=np.array(self._state, dtype=np.float32))\n",
    "        return ts.TimeStep(step_type=StepType.FIRST, \n",
    "                           reward=initial_r, \n",
    "                           discount=np.float32(disc_factor), \n",
    "                           observation=np.array(self._state, dtype=np.float32)\n",
    "                           )\n",
    "    \n",
    "    def set_state(self,new_state):\n",
    "        self._state = new_state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self._state\n",
    "    \n",
    "    def _step(self, action):\n",
    "        '''\n",
    "        The function for the transtion from (S_t, A_t) to (R_{t+1}, S_{t+1}).\n",
    "    \n",
    "        Input.\n",
    "        --------\n",
    "        self: contain S_t.\n",
    "        action: A_t.\n",
    "    \n",
    "        Output.\n",
    "        --------\n",
    "        an TimeStep obj, TimeStep(step_type_{t+1}, R_{t+1}, discount_{t+1}, observation S_{t+1})\n",
    "        ''' \n",
    "        # Suppose that we are at the beginning of time t \n",
    "        \n",
    "        ################## --- Determine whether we should end the episode.\n",
    "        if np.isnan(action).any() or np.isinf(action).any():\n",
    "            print(\"Got invalid action at step\", self._step_counter, \":\", action)\n",
    "            raise ValueError(\"Action is NaN or Inf!\")\n",
    "\n",
    "        \n",
    "        if self._episode_ended:  # its time-t value is set at the end of t-1\n",
    "            return self.reset()\n",
    "        # Move on to the following if self._episode_ended=False\n",
    "        \n",
    "        \n",
    "        ################# --- Compute S_{t+1} \n",
    "        self._state = self._state + action    \n",
    "        self._step_counter +=1\n",
    "        \n",
    "        ################# --- Compute R_{t+1}=R(S_t,A_t)\n",
    "        R = compute_reward(self._state)\n",
    "        if np.isnan(R) or np.isinf(R):\n",
    "            print(\"Reward is invalid at step\", self._step_counter, \"state=\", self._state)\n",
    "            raise ValueError(\"Reward is NaN or Inf!\")\n",
    "        \n",
    "        #Set conditions for termination\n",
    "        if self._step_counter>=sub_episode_length-1:\n",
    "            self._episode_ended = True  #value for t+1\n",
    "\n",
    "        #Now we are at the end of time t, when self._episode_ended may have changed\n",
    "        if self._episode_ended:\n",
    "            #if self._step_counter>100:\n",
    "            #    reward += np.float32(-100)\n",
    "            #ts.termination(observation,reward,outer_dims=None): Returns a TimeStep obj with step_type set to StepType.LAST.\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward=R)\n",
    "        else:\n",
    "            #ts.transition(observation,reward,discount,outer_dims=None): Returns \n",
    "            #a TimeStep obj with step_type set to StepType.MID.\n",
    "            return ts.transition(np.array(self._state, dtype=np.float32), reward=R, discount=disc_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c9068-28cc-41cf-ab83-563b2131cf70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470d9bb-449b-4790-83c6-d1c9f03a90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "ts = env.reset()\n",
    "print(ts)   # should show StepType.FIRST, finite observation & reward=0\n",
    "\n",
    "for _ in range(5):\n",
    "    a = env.action_spec().sample()      # draw a valid random action\n",
    "    ts = env.step(a)\n",
    "    print(ts)  # ensure StepType.MID/LAST, finite obs & reward\n",
    "    if ts.is_last():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fe0a2-3f04-4d69-8106-7a5e35ac7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "timestep = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a70390-a8c6-49b2-a1e6-4656abb7b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f7c9f-f518-4126-85d3-e23d2f983873",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(sub_episode_length + 2):\n",
    "    # sample a random action in the valid range\n",
    "    action = np.random.uniform(act_min, act_max).astype(np.float32)\n",
    "    next_ts = env.step(action)\n",
    "    records.append({\n",
    "        'step': i,\n",
    "        'action': float(action[0]),\n",
    "        'observation': float(next_ts.observation[0]),\n",
    "        'reward': float(next_ts.reward),\n",
    "        'step_type': next_ts.step_type\n",
    "    })\n",
    "    if next_ts.is_last():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79757cf-17ba-45c8-901f-80d9a582aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7aaa7e-6b40-4cd4-8ffa-87cbd23a6446",
   "metadata": {},
   "source": [
    "### End Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ea58e-6f8f-4cff-bbc4-1239b42e0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sequence of parallel environments and batch them, for later use by driver to generate parallel trajectories.\n",
    "parallel_env = ParallelPyEnvironment(env_constructors=[Env]*env_num, \n",
    "                                     start_serially=False,\n",
    "                                     blocking=False,\n",
    "                                     flatten=False\n",
    "                                    )\n",
    "#Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "train_env = tf_py_environment.TFPyEnvironment(parallel_env, check_dims=True) #instance of parallel environments\n",
    "\n",
    "# train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "print('train_env.batch_size = parallel environment number = ', env_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3f11f-820d-4585-b5fe-876b10d88785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                         train_env.observation_spec(),\n",
    "                                         train_env.action_spec(),\n",
    "                                         fc_layer_params=(20,20,20), #Hidden layers\n",
    "                                         seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                         #discrete_projection_net=_categorical_projection_net\n",
    "                                         activation_fn = tf.math.tanh,\n",
    "                                         continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8dc87-73c8-49e9-9a6a-b3387f319410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the  REINFORCE_agent\n",
    "train_step_counter = tf.Variable(0)\n",
    "tf.random.set_seed(0)\n",
    "REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec = train_env.time_step_spec(),\n",
    "        action_spec = train_env.action_spec(),\n",
    "        actor_network = actor_net,\n",
    "        value_network = None,\n",
    "        value_estimation_loss_coef = 0.2,\n",
    "        optimizer = opt,\n",
    "        advantage_fn = None,\n",
    "        use_advantage_loss = False,\n",
    "        gamma = 1.0, #discount factor for future returns\n",
    "        normalize_returns = False, #The instruction says it's better to normalize\n",
    "        gradient_clipping = None,\n",
    "        entropy_regularization = None,\n",
    "        train_step_counter = train_step_counter\n",
    "        )\n",
    "       \n",
    "REINFORCE_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97fd11-7f9f-4c66-8958-e55eb1c86e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#replay_buffer is used to store policy exploration data\n",
    "#################\n",
    "replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                max_length = episode_length*env_num*2    # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#A driver uses an agent to perform its policy in the environment.\n",
    "#The trajectory is saved in replay_buffer\n",
    "collect_driver = DynamicEpisodeDriver(\n",
    "                                     env = train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                     policy = REINFORCE_agent.collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_episodes = sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe475d-a5eb-48f3-884f-11f3d63bda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs.\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                         or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3ba2a-d3c2-480a-b418-9ee13a239256",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################### - 1st line in Table 2 will be Outputed\n",
    "######## Train REINFORCE_agent's actor_network multiple times.\n",
    "update_num = generation_num \n",
    "eval_intv = 100 #number of updates required before each policy evaluation\n",
    "REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta\n",
    "final_reward = -1000\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for n in range(0,update_num):\n",
    "    #Generate Trajectories\n",
    "    replay_buffer.clear()\n",
    "    collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                             tf.zeros_like(rewards),\n",
    "                             tf.zeros_like(discounts),\n",
    "                             observations\n",
    "                            )\n",
    "    \n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = REINFORCE_agent._actor_network.trainable_weights\n",
    "    \n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = REINFORCE_agent.collect_policy.distribution(\n",
    "                               time_steps, policy_state=None).action\n",
    "    \n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                 actions,\n",
    "                                                 REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + param_alpha*regu_term\n",
    "    \n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/sub_episode_length),batch_rewards.argmax()%sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    #print(compute_reward(best_obs,alpha))\n",
    "    if n%eval_intv==0:\n",
    "        print(\"train_step no.=\",train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('episode of rewards', rewards.round(3))\n",
    "        print('act_std:', actions_distribution.stddev()[0,0]  )\n",
    "        print('act_mean:', actions_distribution.mean()[0,0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print(' ')\n",
    "    \n",
    "    if final_reward is None or best_step_reward>final_reward:\n",
    "        print(\"final reward before udpate:\",final_reward)\n",
    "        final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        print(\"final reward after udpate:\",final_reward)\n",
    "        print('updated final_solution=', final_solution)\n",
    "        \n",
    "print('final_solution=',final_solution,\n",
    "      'final_reward=',final_reward,\n",
    "     )\n",
    "REINFORCE_logs = [max(REINFORCE_logs[0:i]) for i in range(1,generation_num+1)] #rolling max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36467ed-be0a-48b2-b4f1-4f2f44985517",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdf33d-d75e-4d3e-95ad-db58fff86f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475251fe-b797-4712-993f-c70a4d97c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
